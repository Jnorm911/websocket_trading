{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports** üïµÔ∏è‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from scipy.signal import detrend\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions** ü§å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(X, y, time_steps=60):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def check_data_balance(y):\n",
    "    counter = Counter(y)\n",
    "    for label, count in counter.items():\n",
    "        percentage = (count / len(y)) * 100\n",
    "        print(f\"Class {label}: {count} samples ({percentage:.2f}%)\")\n",
    "\n",
    "# Preprocessing function for train and test sets\n",
    "def preprocess(df):\n",
    "    # Copy the dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Fill NaNs in specific columns with 0\n",
    "    df['PSARl_0.01_0.1'] = df['PSARl_0.01_0.1'].fillna(0)\n",
    "    df['PSARs_0.01_0.1'] = df['PSARs_0.01_0.1'].fillna(0)\n",
    "\n",
    "    # Identify the first non-null row\n",
    "    first_valid_index = df.dropna().index[0]\n",
    "\n",
    "    # Drop the rows before this index\n",
    "    df = df.loc[first_valid_index:]\n",
    "\n",
    "    # Use ffill to fill any remaining missing values\n",
    "    df.ffill(inplace=True)\n",
    "\n",
    "    # Return the processed dataframe\n",
    "    return df\n",
    "\n",
    "# Separate function for feature engineering\n",
    "def feature_engineering(df, lag_and_window_features):\n",
    "    # Convert DataFrame to numpy arrays for faster computation\n",
    "    features_np = df[lag_and_window_features].to_numpy()\n",
    "    n_samples, n_features = features_np.shape\n",
    "    engineered_features = {}\n",
    "\n",
    "    # Lagged Features\n",
    "    for i, feature in enumerate(lag_and_window_features):\n",
    "        for lag in [1, 2, 3, 5, 10]:\n",
    "            lagged = np.roll(features_np[:, i], lag)\n",
    "            lagged[:lag] = np.nan  # Pad with NaN\n",
    "            engineered_features[f'{feature}_lag_{lag}'] = lagged\n",
    "\n",
    "    # Rolling Window Statistics\n",
    "    for i, feature in enumerate(lag_and_window_features):\n",
    "        for window in [3, 5, 10]:\n",
    "            rolled = np.lib.stride_tricks.sliding_window_view(features_np[:, i], window)\n",
    "            rolled_mean = np.full(n_samples, np.nan)\n",
    "            rolled_mean[window-1:] = np.mean(rolled, -1)  # Compute mean\n",
    "            rolled_std = np.full(n_samples, np.nan)\n",
    "            rolled_std[window-1:] = np.std(rolled, -1)  # Compute standard deviation\n",
    "            engineered_features[f'{feature}_rolling_mean_{window}'] = rolled_mean\n",
    "            engineered_features[f'{feature}_rolling_std_{window}'] = rolled_std\n",
    "\n",
    "    # Convert engineered features back to DataFrame\n",
    "    engineered_features_df = pd.DataFrame(engineered_features, index=df.index)\n",
    "\n",
    "    # Concatenate original features with engineered features\n",
    "    df = pd.concat([df, engineered_features_df], axis=1)\n",
    "\n",
    "    # Drop rows with NaN values created by lagged features and rolling window statistics\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Globals** üåé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "feature_names = [\n",
    "    # Fundamental price data\n",
    "    'open', 'high', 'low', 'close', \n",
    "\n",
    "    # Auxiliary data\n",
    "    'turnover', 'color',\n",
    "\n",
    "    # Volume-related\n",
    "    'volume', 'avg_vol_last_100', 'obv', \n",
    "\n",
    "    # Momentum and trend indicators\n",
    "    'RSI_5', 'RSI_10', 'RSI_14',\n",
    "    'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'MACD_6_13_5_6_13_5', 'MACDh_6_13_5_6_13_5', 'MACDs_6_13_5_6_13_5', \n",
    "\n",
    "    # Moving averages\n",
    "    'SMA_20', 'SMA_5', 'SMA_10', 'EMA_2', 'EMA_5', 'EMA_10', \n",
    "\n",
    "    # Bollinger Bands\n",
    "    'BBP_10_2.0_10', 'BBL_15_2.0_15', 'BBM_15_2.0_15', 'BBU_15_2.0_15', 'BBB_15_2.0_15', 'BBP_15_2.0_15', 'BBL_20_2.0_20', 'BBM_20_2.0_20', 'BBU_20_2.0_20', 'BBB_20_2.0_20', 'BBP_20_2.0_20', 'bollinger_bandwidth',\n",
    "    'BBL_5_2.0_5', 'BBM_5_2.0_5', 'BBU_5_2.0_5', 'BBB_5_2.0_5', 'BBP_5_2.0_5', 'BBL_10_2.0_10', 'BBM_10_2.0_10', 'BBU_10_2.0_10', 'BBB_10_2.0_10', 'BBP_10_2.0_10',\n",
    "\n",
    "    # Stochastic Oscillator\n",
    "    'STOCHd_14_3_3', 'STOCHk_14_3_3_7_3_3', 'STOCHd_14_3_3_7_3_3', 'STOCHk_14_3_3_10_3_3', 'STOCHd_14_3_3_10_3_3', \n",
    "\n",
    "    # Volatility\n",
    "    'ATR_14', 'ATR_10', 'ATR_5', \n",
    "\n",
    "    # Other momentum oscillators\n",
    "    'ROC_14', 'ROC_10', 'ROC_5', \n",
    "\n",
    "    # Other versatile indicators\n",
    "    'CCI_14', 'CCI_10', 'CCI_5', \n",
    "\n",
    "    # Money Flow Index and Chaikin Money Flow\n",
    "    'cmf', 'mfi',\n",
    "    \n",
    "    # Relative Vigor Index (RVI)\n",
    "    'RVI_15', 'RVI_10', 'RVI_5',\n",
    "    \n",
    "    # Pivot Points\n",
    "    'PP', 'R1', 'S1', 'R2', 'S2', 'R3', 'S3',\n",
    "\n",
    "    # Parabolic SAR (PSAR)\n",
    "    'PSARl_0.01_0.1', 'PSARs_0.01_0.1', 'PSARaf_0.01_0.1', 'PSARr_0.01_0.1',\n",
    "\n",
    "    # Triple Exponential Average (TRIX)\n",
    "    'TRIX_18_9', 'TRIXs_18_9', 'TRIX_12_6', 'TRIXs_12_6', 'TRIX_10_5', 'TRIXs_10_5',\n",
    "\n",
    "    # Ichimoku Cloud (ISA, ISB, ITS, IKS, ICS)\n",
    "    'ISA_5', 'ISB_15', 'ITS_5', 'IKS_15', 'ICS_15',\n",
    "\n",
    "]\n",
    "\n",
    "# List of original features you have\n",
    "lag_and_window_features = ['open', 'high', 'low', 'close', 'volume', 'SMA_20', 'SMA_5', 'SMA_10', 'ROC_14', 'ROC_10', 'ROC_5', \n",
    "                'RSI_5', 'RSI_10', 'RSI_14', 'BBP_10_2.0_10', 'BBL_15_2.0_15', 'BBM_15_2.0_15', 'BBU_15_2.0_15', \n",
    "                'BBB_15_2.0_15', 'BBP_15_2.0_15', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'STOCHk_14_3_3', \n",
    "                'STOCHd_14_3_3', 'obv', 'ATR_14', 'ATR_10', 'ATR_5', 'color', 'avg_vol_last_100']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing** üëª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1.0: 866 samples (53.99%)\n",
      "Class 0.0: 738 samples (46.01%)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../../../data/kc/btc/heiken_ashi/with_trade_indicators/raw/kc_btc_15min_ha_ti.csv')\n",
    "\n",
    "# Convert color to 0 for 'red' and 1 for 'green'\n",
    "df['color'] = df['color'].map({'red': 0, 'green': 1})\n",
    "\n",
    "# Add 'color_change' column: 1 if color changes from the previous row, 0 otherwise\n",
    "df['color_change'] = df['color'].diff().abs()\n",
    "\n",
    "# Fill the first row's 'color_change' with 0\n",
    "df['color_change'].fillna(0, inplace=True)\n",
    "\n",
    "# Drop 'time' and 'turnover' columns\n",
    "df = df.drop(['time'], axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "features_df = df.drop('color_change', axis=1)\n",
    "target = df['color_change']\n",
    "\n",
    "# Determine the split point\n",
    "split_point = int(len(features_df) * 0.8)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train_df, X_test_df = features_df[:split_point], features_df[split_point:]\n",
    "y_train, y_test = target[:split_point], target[split_point:]\n",
    "\n",
    "# Apply preprocessing to the train and test sets\n",
    "X_train_df = preprocess(X_train_df)\n",
    "X_test_df = preprocess(X_test_df)\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train = scaler.fit_transform(X_train_df)\n",
    "X_test = scaler.transform(X_test_df)\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features] for LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "check_data_balance(y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering** üöÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering to the train and test sets\n",
    "X_train_df = feature_engineering(X_train_df, lag_and_window_features)\n",
    "X_test_df = feature_engineering(X_test_df, lag_and_window_features)\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train = scaler.fit_transform(X_train_df)\n",
    "X_test = scaler.transform(X_test_df)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "X_train = pd.DataFrame(X_train, columns=X_train_df.columns, index=X_train_df.index)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test_df.columns, index=X_test_df.index)\n",
    "\n",
    "# The target needs to match the features DataFrame\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_test = y_test.loc[X_test.index]\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features] for LSTM\n",
    "X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "check_data_balance(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Fit a Logistic Regression model to the data\n",
    "log_reg = LogisticRegression(C=1, penalty='l1', solver='liblinear', random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Use SelectFromModel to select features whose coefficients are non-zero\n",
    "selector = SelectFromModel(estimator=log_reg, prefit=True)\n",
    "\n",
    "# Transform the data\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get the selected feature names\n",
    "selected_features = features_df.columns[selector.get_support()]\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation** üè¥‚Äç‚ò†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "201/201 [==============================] - 2s 2ms/step - loss: 0.7212 - accuracy: 0.5507 - val_loss: 0.6738 - val_accuracy: 0.6041\n",
      "Epoch 2/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.6186 - accuracy: 0.6794 - val_loss: 0.6009 - val_accuracy: 0.6827\n",
      "Epoch 3/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.5531 - accuracy: 0.7322 - val_loss: 0.5846 - val_accuracy: 0.7319\n",
      "Epoch 4/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.7495 - val_loss: 0.5654 - val_accuracy: 0.7307\n",
      "Epoch 5/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.5649 - accuracy: 0.7350 - val_loss: 0.8164 - val_accuracy: 0.6739\n",
      "Epoch 6/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7544 - val_loss: 0.5237 - val_accuracy: 0.7519\n",
      "Epoch 7/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.5618 - accuracy: 0.7313 - val_loss: 0.6788 - val_accuracy: 0.5817\n",
      "Epoch 8/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.7145 - val_loss: 0.5569 - val_accuracy: 0.7251\n",
      "Epoch 9/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.7555 - val_loss: 0.5201 - val_accuracy: 0.7444\n",
      "Epoch 10/10\n",
      "201/201 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.7484 - val_loss: 0.5364 - val_accuracy: 0.7450\n",
      "51/51 [==============================] - 0s 983us/step - loss: 0.5364 - accuracy: 0.7450\n",
      "Test Accuracy: 74.50\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Accuracy: %.2f' % (accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

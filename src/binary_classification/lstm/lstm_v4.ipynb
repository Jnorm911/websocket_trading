{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as pta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Convert 'color' column to binary representation\n",
    "    df[\"color\"] = df[\"color\"].map({\"red\": 0, \"green\": 1})\n",
    "\n",
    "    df[\"color_change\"] = df[\"color\"].diff().ne(0).astype(int)\n",
    "    df[\"color_change\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Fill NaNs in specified columns with 0\n",
    "    df[\"PSARl_0.01_0.1\"].fillna(0, inplace=True)\n",
    "    df[\"PSARs_0.01_0.1\"].fillna(0, inplace=True)\n",
    "    df[\"ICS_15\"].fillna(0, inplace=True)\n",
    "\n",
    "    # # Diff features\n",
    "    # df[\"volume_diff\"] = df[\"volume\"].diff()\n",
    "    # df[\"turnover_diff\"] = df[\"turnover\"].diff()\n",
    "    # df[\"close_open\"] = df[\"close\"] - df[\"open\"]\n",
    "    # df[\"open_close\"] = df[\"open\"] - df[\"close\"]\n",
    "    # df[\"high_low\"] = df[\"high\"] - df[\"low\"]\n",
    "    df[\"close_change\"] = df[\"close\"].diff()\n",
    "    # df[\"open_change\"] = df[\"open\"].diff()\n",
    "    # df[\"h_diff\"] = df[\"high\"] - df[\"high\"].shift(1)\n",
    "    # df[\"l_diff\"] = df[\"low\"] - df[\"low\"].shift(1)\n",
    "    # df[\"hl_shift\"] = df[\"high\"] - df[\"low\"].shift(1)\n",
    "    df[\"high_pct\"] = df[\"high\"].pct_change()\n",
    "    # df[\"low_pct\"] = df[\"low\"].pct_change()\n",
    "    # df[\"close_open_lag\"] = df[\"close_open\"].shift(1)\n",
    "    # df[\"high_low_lag\"] = df[\"high_low\"].shift(1)\n",
    "\n",
    "    # # Ratio features\n",
    "    # N = 10\n",
    "    # df[\"PSARr_to_RVI\"] = df[\"PSARr_0.01_0.1\"] / df[\"RVI_5\"]\n",
    "    # df[\"High_low_pct_diff\"] = df[\"high_pct\"] - df[\"low_pct\"]\n",
    "    # consecutive_same_color = (\n",
    "    #     (df[\"color\"] == df[\"color\"].shift(1))\n",
    "    #     .astype(int)\n",
    "    #     .groupby(df[\"color\"].ne(df[\"color\"].shift()).cumsum())\n",
    "    #     .cumcount()\n",
    "    # )\n",
    "    # df[\"past_candle_color_ratio\"] = consecutive_same_color.rolling(N).sum() / N\n",
    "    # df[\"volume_to_avg_vol_ratio\"] = df[\"volume\"] / df[\"avg_vol_last_100\"]\n",
    "    # df[\"turnover_to_avg_turnover_ratio\"] = (\n",
    "    #     df[\"turnover\"] / df[\"turnover\"].rolling(N).mean()\n",
    "    # )\n",
    "    # upper_shadow = (df[\"high\"] - df[[\"open\", \"close\"]].max(axis=1)).apply(abs)\n",
    "    # lower_shadow = (df[\"low\"] - df[[\"open\", \"close\"]].min(axis=1)).apply(abs)\n",
    "    # df[\"shadow_ratio\"] = upper_shadow / lower_shadow\n",
    "    # df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # # Lagged features\n",
    "    # df[\"lag5_high_pct\"] = df[\"high_pct\"].shift(5)\n",
    "    # df[\"lag5_close_change\"] = df[\"close_change\"].shift(5)\n",
    "    # df[\"lag5_close_change\"] = df[\"close_change\"].shift(1)\n",
    "\n",
    "    # Rolling features\n",
    "    df[\"close_change_roll5\"] = df[\"close_change\"].rolling(20).mean()\n",
    "    df[\"RSI_14_roll5\"] = df[\"RSI_14\"].rolling(20).mean()\n",
    "    df[\"ATR_14_roll5\"] = df[\"ATR_14\"].rolling(20).mean()\n",
    "    df[\"volume_roll5\"] = df[\"volume\"].rolling(20).mean()\n",
    "    df[\"high_pct_roll5\"] = df[\"high_pct\"].rolling(20).mean()\n",
    "    df[\"volatility_5\"] = df[\"close\"].rolling(20).std()\n",
    "    df[\"price_ema5\"] = df[\"close\"].ewm(span=20).mean()\n",
    "    df[\"volume_ema5\"] = df[\"volume\"].ewm(span=20).mean()\n",
    "    df[\"price_to_ema5\"] = df[\"close\"] / df[\"price_ema5\"] - 1\n",
    "    df[\"volume_change_roll5\"] = df[\"volume\"].pct_change().rolling(20).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_data(df):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return df_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set display options to show all rows and columns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Load the data\n",
    "data_path = (\n",
    "    \"../../../data/kc/btc/heiken_ashi/with_trade_indicators/raw/kc_btc_12min_ha_ti.csv\"\n",
    ")\n",
    "\n",
    "# List of features to drop\n",
    "features_to_drop = [\n",
    "    ########\n",
    "    # Univariate Feature Selection Features #\n",
    "    #######\n",
    "    \"cmf\",\n",
    "    \"MACDs_6_13_5_6_13_5\",\n",
    "    \"RVI_10\",\n",
    "    \"BBL_5_2.0_5\",\n",
    "    \"STOCHk_14_3_3\",\n",
    "    \"STOCHk_14_3_3_7_3_3\",\n",
    "    \"STOCHk_14_3_3_10_3_3\",\n",
    "    \"MACDh_6_13_5_6_13_5\",\n",
    "    \"MACDh_12_26_9\",\n",
    "    \"BBL_10_2.0_10\",\n",
    "    \"R3\",\n",
    "    \"R2\",\n",
    "    \"R1\",\n",
    "    \"open\",\n",
    "    \"BBL_15_2.0_15\",\n",
    "    \"SMA_5\",\n",
    "    \"BBM_5_2.0_5\",\n",
    "    \"PP\",\n",
    "    \"BBL_20_2.0_20\",\n",
    "    \"EMA_5\",\n",
    "    \"ITS_5\",\n",
    "    \"SMA_10\",\n",
    "    \"BBM_10_2.0_10\",\n",
    "    \"EMA_10\",\n",
    "    \"S1\",\n",
    "    \"SMA_20\",\n",
    "    \"EMA_2\",\n",
    "    \"BBM_15_2.0_15\",\n",
    "    \"S2\",\n",
    "    \"BBM_20_2.0_20\",\n",
    "    \"IKS_15\",\n",
    "    \"close\",\n",
    "    \"PSARaf_0.01_0.1\",\n",
    "    \"S3\",\n",
    "    \"MACD_6_13_5_6_13_5\",\n",
    "    \"ISA_5\",\n",
    "    \"ISB_15\",\n",
    "    \"BBU_15_2.0_15\",\n",
    "    \"BBU_10_2.0_10\",\n",
    "    \"BBB_15_2.0_15\",\n",
    "    \"obv\",\n",
    "    \"BBU_5_2.0_5\",\n",
    "    \"high\",\n",
    "    \"STOCHd_14_3_3\",\n",
    "    \"STOCHd_14_3_3_7_3_3\",\n",
    "    \"STOCHd_14_3_3_10_3_3\",\n",
    "    \"TRIXs_10_5\",\n",
    "    \"MACD_12_26_9\",\n",
    "    \"PSARs_0.01_0.1\",\n",
    "    \"TRIXs_18_9\",\n",
    "    \"ICS_15\",\n",
    "    \"ROC_10\",\n",
    "    \"BBB_20_2.0_20\",\n",
    "    \"bollinger_bandwidth\",\n",
    "    \"RVI_15\",\n",
    "    \"MACDs_12_26_9\",\n",
    "    \"PSARl_0.01_0.1\",\n",
    "    \"TRIX_18_9\",\n",
    "    \"TRIXs_12_6\",\n",
    "    \"ROC_5\",\n",
    "    \"BBU_20_2.0_20\",\n",
    "    \"low\",\n",
    "    \"TRIX_12_6\",\n",
    "    \"ROC_14\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../data/kc/btc/heiken_ashi/with_trade_indicators/raw/kc_btc_12min_ha_ti.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m load_data(data_path)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Preprocess the data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df \u001b[39m=\u001b[39m preprocess_data(df)\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(data_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m(data_path):\n\u001b[1;32m----> 2\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(data_path)\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/kc/btc/heiken_ashi/with_trade_indicators/raw/kc_btc_12min_ha_ti.csv'"
     ]
    }
   ],
   "source": [
    "df = load_data(data_path)\n",
    "\n",
    "# Preprocess the data\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Drop the specified features\n",
    "df = df.drop(columns=features_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Prepare TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Forward Fill\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[39m.\u001b[39mffill(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Backward Fill\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df\u001b[39m.\u001b[39mbfill(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Forward Fill\n",
    "df.ffill(inplace=True)\n",
    "\n",
    "# Backward Fill\n",
    "df.bfill(inplace=True)\n",
    "\n",
    "# Re-scale the data to include the new feature\n",
    "df_scaled = scale_data(df)\n",
    "\n",
    "# print(df.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define initial features for ablation process\n",
    "# ablation_features = [\n",
    "#     \"cmf\",\n",
    "#     \"MACDs_6_13_5_6_13_5\",\n",
    "#     \"RVI_10\",\n",
    "#     \"BBL_5_2.0_5\",\n",
    "#     \"STOCHk_14_3_3\",\n",
    "#     \"STOCHk_14_3_3_7_3_3\",\n",
    "#     \"STOCHk_14_3_3_10_3_3\",\n",
    "#     \"MACDh_6_13_5_6_13_5\",\n",
    "#     \"MACDh_12_26_9\",\n",
    "#     \"BBL_10_2.0_10\",\n",
    "#     \"R3\",\n",
    "#     \"R2\",\n",
    "#     \"o_shift\",\n",
    "#     \"R1\",\n",
    "#     \"open\",\n",
    "#     \"BBL_15_2.0_15\",\n",
    "#     \"SMA_5\",\n",
    "#     \"BBM_5_2.0_5\",\n",
    "#     \"c_shift\",\n",
    "#     \"PP\",\n",
    "#     \"BBL_20_2.0_20\",\n",
    "#     \"EMA_5\",\n",
    "#     \"ITS_5\",\n",
    "#     \"SMA_10\",\n",
    "#     \"BBM_10_2.0_10\",\n",
    "#     \"EMA_10\",\n",
    "#     \"S1\",\n",
    "#     \"SMA_20\",\n",
    "#     \"EMA_2\",\n",
    "#     \"BBM_15_2.0_15\",\n",
    "#     \"S2\",\n",
    "#     \"BBM_20_2.0_20\",\n",
    "#     \"IKS_15\",\n",
    "#     \"close\",\n",
    "#     \"PSARaf_0.01_0.1\",\n",
    "#     \"S3\",\n",
    "#     \"MACD_6_13_5_6_13_5\",\n",
    "#     \"ISA_5\",\n",
    "#     \"ISB_15\",\n",
    "#     \"BBU_15_2.0_15\",\n",
    "#     \"BBU_10_2.0_10\",\n",
    "#     \"BBB_15_2.0_15\",\n",
    "#     \"obv\",\n",
    "#     \"BBU_5_2.0_5\",\n",
    "#     \"high\",\n",
    "#     \"STOCHd_14_3_3\",\n",
    "#     \"STOCHd_14_3_3_7_3_3\",\n",
    "#     \"STOCHd_14_3_3_10_3_3\",\n",
    "#     \"TRIXs_10_5\",\n",
    "#     \"MACD_12_26_9\",\n",
    "#     \"PSARs_0.01_0.1\",\n",
    "#     \"TRIXs_18_9\",\n",
    "#     \"ICS_15\",\n",
    "#     \"ROC_10\",\n",
    "#     \"BBB_20_2.0_20\",\n",
    "#     \"bollinger_bandwidth\",\n",
    "#     \"RVI_15\",\n",
    "#     \"MACDs_12_26_9\",\n",
    "#     \"PSARl_0.01_0.1\",\n",
    "#     \"TRIX_18_9\",\n",
    "#     \"volume_diff_lag\",\n",
    "#     \"TRIXs_12_6\",\n",
    "#     \"ROC_5\",\n",
    "#     \"BBU_20_2.0_20\",\n",
    "#     \"low\",\n",
    "#     \"TRIX_12_6\",\n",
    "#     \"ROC_14\",\n",
    "# ]\n",
    "\n",
    "# # All the features in the data\n",
    "# all_features = list(df.columns)\n",
    "# all_features.remove(\"color_change\")  # we remove the target variable\n",
    "\n",
    "# log_reg_scores = {}\n",
    "# rf_scores = {}\n",
    "\n",
    "# for feature in ablation_features:\n",
    "#     # Remove one feature from the all features list\n",
    "#     current_features = [f for f in all_features if f != feature]\n",
    "\n",
    "#     # Prepare your features and target\n",
    "#     X = df[current_features]\n",
    "#     y = df[\"color_change\"]\n",
    "\n",
    "#     # Logistic Regression\n",
    "#     log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(log_reg, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "#     log_reg_scores[feature] = np.mean(cv_scores)\n",
    "\n",
    "# # Print logistic regression scores sorted by score\n",
    "# print(\"Logistic Regression Scores\")\n",
    "# for feature, score in sorted(\n",
    "#     log_reg_scores.items(), key=lambda item: item[1], reverse=True\n",
    "# ):\n",
    "#     print(f\"CV ROC AUC score without {feature}: {score}\")\n",
    "\n",
    "# for feature in ablation_features:\n",
    "#     # Remove one feature from the all features list\n",
    "#     current_features = [f for f in all_features if f != feature]\n",
    "\n",
    "#     # Prepare your features and target\n",
    "#     X = df[current_features]\n",
    "#     y = df[\"color_change\"]\n",
    "\n",
    "#     # Random Forest\n",
    "#     rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(rf, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "#     rf_scores[feature] = np.mean(cv_scores)\n",
    "\n",
    "# # Print random forest scores sorted by score\n",
    "# print(\"\\nRandom Forest Scores\")\n",
    "# for feature, score in sorted(rf_scores.items(), key=lambda item: item[1], reverse=True):\n",
    "#     print(f\"CV ROC AUC score without {feature}: {score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Ablation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define initial features for ablation process\n",
    "# ablation_features = [\n",
    "#     \"cmf\",\n",
    "#     \"MACDs_6_13_5_6_13_5\",\n",
    "#     \"RVI_10\",\n",
    "#     \"BBL_5_2.0_5\",\n",
    "#     \"STOCHk_14_3_3\",\n",
    "#     \"STOCHk_14_3_3_7_3_3\",\n",
    "#     \"STOCHk_14_3_3_10_3_3\",\n",
    "#     \"MACDh_6_13_5_6_13_5\",\n",
    "#     \"MACDh_12_26_9\",\n",
    "#     \"BBL_10_2.0_10\",\n",
    "#     \"R3\",\n",
    "#     \"R2\",\n",
    "#     \"o_shift\",\n",
    "#     \"R1\",\n",
    "#     \"open\",\n",
    "#     \"BBL_15_2.0_15\",\n",
    "#     \"SMA_5\",\n",
    "#     \"BBM_5_2.0_5\",\n",
    "#     \"c_shift\",\n",
    "#     \"PP\",\n",
    "#     \"BBL_20_2.0_20\",\n",
    "#     \"EMA_5\",\n",
    "#     \"ITS_5\",\n",
    "#     \"SMA_10\",\n",
    "#     \"BBM_10_2.0_10\",\n",
    "#     \"EMA_10\",\n",
    "#     \"S1\",\n",
    "#     \"SMA_20\",\n",
    "#     \"EMA_2\",\n",
    "#     \"BBM_15_2.0_15\",\n",
    "#     \"S2\",\n",
    "#     \"BBM_20_2.0_20\",\n",
    "#     \"IKS_15\",\n",
    "#     \"close\",\n",
    "#     \"PSARaf_0.01_0.1\",\n",
    "#     \"S3\",\n",
    "#     \"MACD_6_13_5_6_13_5\",\n",
    "#     \"ISA_5\",\n",
    "#     \"ISB_15\",\n",
    "#     \"BBU_15_2.0_15\",\n",
    "#     \"BBU_10_2.0_10\",\n",
    "#     \"BBB_15_2.0_15\",\n",
    "#     \"obv\",\n",
    "#     \"BBU_5_2.0_5\",\n",
    "#     \"high\",\n",
    "#     \"STOCHd_14_3_3\",\n",
    "#     \"STOCHd_14_3_3_7_3_3\",\n",
    "#     \"STOCHd_14_3_3_10_3_3\",\n",
    "#     \"TRIXs_10_5\",\n",
    "#     \"MACD_12_26_9\",\n",
    "#     \"PSARs_0.01_0.1\",\n",
    "#     \"TRIXs_18_9\",\n",
    "#     \"ICS_15\",\n",
    "#     \"ROC_10\",\n",
    "#     \"BBB_20_2.0_20\",\n",
    "#     \"bollinger_bandwidth\",\n",
    "#     \"RVI_15\",\n",
    "#     \"MACDs_12_26_9\",\n",
    "#     \"PSARl_0.01_0.1\",\n",
    "#     \"TRIX_18_9\",\n",
    "#     \"volume_diff_lag\",\n",
    "#     \"TRIXs_12_6\",\n",
    "#     \"ROC_5\",\n",
    "#     \"BBU_20_2.0_20\",\n",
    "#     \"low\",\n",
    "#     \"TRIX_12_6\",\n",
    "#     \"ROC_14\",\n",
    "# ]\n",
    "\n",
    "# # All the features in the data\n",
    "# all_features = list(df.columns)\n",
    "# all_features.remove(\"color_change\")  # we remove the target variable\n",
    "\n",
    "# # Base features are all features excluding the ones in the ablation list\n",
    "# base_features = [\n",
    "#     feature for feature in all_features if feature not in ablation_features\n",
    "# ]\n",
    "\n",
    "# log_reg_scores = {}\n",
    "# rf_scores = {}\n",
    "\n",
    "# for feature in ablation_features:\n",
    "#     # Add one feature from the ablation list\n",
    "#     current_features = base_features + [feature]\n",
    "\n",
    "#     # Prepare your features and target\n",
    "#     X = df[current_features]\n",
    "#     y = df[\"color_change\"]\n",
    "\n",
    "#     # Logistic Regression\n",
    "#     log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(log_reg, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "#     log_reg_scores[feature] = np.mean(cv_scores)\n",
    "\n",
    "# # Print logistic regression scores sorted by score\n",
    "# print(\"Logistic Regression Scores\")\n",
    "# for feature, score in sorted(\n",
    "#     log_reg_scores.items(), key=lambda item: item[1], reverse=True\n",
    "# ):\n",
    "#     print(f\"CV ROC AUC score with {feature}: {score}\")\n",
    "\n",
    "# for feature in ablation_features:\n",
    "#     # Add one feature from the ablation list\n",
    "#     current_features = base_features + [feature]\n",
    "\n",
    "#     # Prepare your features and target\n",
    "#     X = df[current_features]\n",
    "#     y = df[\"color_change\"]\n",
    "\n",
    "#     # Random Forest\n",
    "#     rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(rf, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "#     rf_scores[feature] = np.mean(cv_scores)\n",
    "\n",
    "# # Print random forest scores sorted by score\n",
    "# print(\"\\nRandom Forest Scores\")\n",
    "# for feature, score in sorted(rf_scores.items(), key=lambda item: item[1], reverse=True):\n",
    "#     print(f\"CV ROC AUC score with {feature}: {score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(\"color_change\", axis=1)\n",
    "# y = df[\"color_change\"]\n",
    "\n",
    "# # Re-scale the data to include the new feature\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Fit XGBoost model and get feature importances\n",
    "# xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "# xgb.fit(X_scaled, y)\n",
    "\n",
    "# # Store the feature importances in a pandas series, then sort it in descending order\n",
    "# importances = pd.Series(xgb.feature_importances_, index=X.columns)\n",
    "# importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# print(\"Feature importances:\")\n",
    "# print(importances_sorted)\n",
    "\n",
    "# # Now we can apply Logistic Regression and Random Forests using the features\n",
    "# # Logistic Regression\n",
    "# log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# # Cross-validation\n",
    "# cv_scores = cross_val_score(log_reg, X_scaled, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "# print(f\"\\nLogistic Regression CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# # Random Forest\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Cross-validation\n",
    "# cv_scores = cross_val_score(rf, X_scaled, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "# print(f\"Random Forest CV F1 score: {np.mean(cv_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(\"color_change\", axis=1)\n",
    "# y = df[\"color_change\"]\n",
    "\n",
    "# # Re-scale the data to include the new feature\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Recursive Feature Elimination\n",
    "# # Here we use Logistic Regression as the model to evaluate the feature importance\n",
    "# # You can replace it with any model you prefer\n",
    "# model = LogisticRegression(random_state=42, max_iter=500)\n",
    "# rfe = RFECV(model)\n",
    "# rfe.fit(X_scaled, y)\n",
    "\n",
    "# # Get the features ranking\n",
    "# feature_ranking = {\n",
    "#     feature_name: rank for feature_name, rank in zip(X.columns, rfe.ranking_)\n",
    "# }\n",
    "\n",
    "# # Sort and print the feature ranking\n",
    "# for feature_name, rank in sorted(feature_ranking.items(), key=lambda item: item[1]):\n",
    "#     print(f\"{feature_name}: {rank}\")\n",
    "\n",
    "# # Transform X to include only the selected features\n",
    "# X_transformed = rfe.transform(X_scaled)\n",
    "\n",
    "# # Logistic Regression with transformed features\n",
    "# log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# # Cross-validation\n",
    "# cv_scores = cross_val_score(log_reg, X_transformed, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "# print(f\"\\nLogistic Regression CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# # Random Forest with transformed features\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Cross-validation\n",
    "# cv_scores = cross_val_score(rf, X_transformed, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "# print(f\"Random Forest CV F1 score: {np.mean(cv_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection Process ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_pct: 110.94480524940894\n",
      "volume: 91.92131389069183\n",
      "turnover: 89.7339138964675\n",
      "BBB_5_2.0_5: 40.51976375637065\n",
      "PSARr_0.01_0.1: 33.222447998121424\n",
      "ATR_5: 8.09442137040173\n",
      "volume_ema5: 7.443922940784361\n",
      "BBP_15_2.0_15: 6.265493240326674\n",
      "CCI_14: 6.182127667059803\n",
      "CCI_10: 5.536539624921639\n",
      "close_change: 5.264615289857336\n",
      "BBP_10_2.0_10: 4.8425351451218335\n",
      "volume_change_roll5: 4.467484190456277\n",
      "BBP_20_2.0_20: 4.391442577137113\n",
      "BBB_10_2.0_10: 3.449706673320239\n",
      "volume_roll5: 3.405378790575545\n",
      "ATR_10: 3.3469011580871424\n",
      "mfi: 2.7913408994283717\n",
      "color: 2.650632704450662\n",
      "ATR_14: 2.5343458390778144\n",
      "BBP_5_2.0_5: 2.5087912415826117\n",
      "CCI_5: 2.4255515533899725\n",
      "avg_vol_last_100: 2.179824307137989\n",
      "RSI_5: 1.8759432547309074\n",
      "RVI_5: 1.770072600613789\n",
      "RSI_10: 1.700735361160013\n",
      "RSI_14: 1.3811647003530192\n",
      "time: 1.285269957412531\n",
      "high_pct_roll5: 1.2662544938393032\n",
      "ATR_14_roll5: 0.8521782525282505\n",
      "price_ema5: 0.31805785232331435\n",
      "TRIX_10_5: 0.31578147717879135\n",
      "RSI_14_roll5: 0.3015932283688058\n",
      "close_change_roll5: 0.12167940381357349\n",
      "price_to_ema5: 0.10717434509139667\n",
      "volatility_5: 0.10047315367559004\n",
      "\n",
      "Logistic Regression CV F1 score: 0.6692820572508955\n",
      "Random Forest CV F1 score: 0.8100051138958743\n",
      "\n",
      " Index(['time', 'volume', 'turnover', 'color', 'avg_vol_last_100', 'RSI_5',\n",
      "       'RSI_10', 'RSI_14', 'ATR_14', 'ATR_10', 'ATR_5', 'CCI_14', 'CCI_10',\n",
      "       'CCI_5', 'mfi', 'RVI_5', 'PSARr_0.01_0.1', 'TRIX_10_5', 'BBB_5_2.0_5',\n",
      "       'BBP_5_2.0_5', 'BBB_10_2.0_10', 'BBP_10_2.0_10', 'BBP_15_2.0_15',\n",
      "       'BBP_20_2.0_20', 'close_change', 'high_pct', 'close_change_roll5',\n",
      "       'RSI_14_roll5', 'ATR_14_roll5', 'volume_roll5', 'high_pct_roll5',\n",
      "       'volatility_5', 'price_ema5', 'volume_ema5', 'price_to_ema5',\n",
      "       'volume_change_roll5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(\"color_change\", axis=1)\n",
    "y = df[\"color_change\"]\n",
    "\n",
    "# Re-scale the data to include the new feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "selector.fit(X_scaled, y)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "features_df_new = X.iloc[:, cols]\n",
    "\n",
    "# Store the scores of each feature in a dictionary\n",
    "feature_scores = {\n",
    "    feature_name: score for feature_name, score in zip(X.columns, selector.scores_)\n",
    "}\n",
    "\n",
    "# Sort the dictionary by value in descending order and print the scores\n",
    "for feature_name, score in sorted(\n",
    "    feature_scores.items(), key=lambda item: item[1], reverse=True\n",
    "):\n",
    "    print(f\"{feature_name}: {score}\")\n",
    "\n",
    "# Now we can apply Logistic Regression and Random Forests on the new features_df_new\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, features_df_new, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "print(f\"\\nLogistic Regression CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, features_df_new, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "print(f\"Random Forest CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "print(\"\\n\", features_df_new.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV ROC AUC score: 0.5547196931554365\n",
      "Random Forest CV ROC AUC score: 0.8702600316865063\n",
      "\n",
      " Index(['time', 'volume', 'turnover', 'color', 'avg_vol_last_100', 'RSI_5',\n",
      "       'RSI_10', 'RSI_14', 'ATR_14', 'ATR_10', 'ATR_5', 'CCI_14', 'CCI_10',\n",
      "       'CCI_5', 'mfi', 'RVI_5', 'PSARr_0.01_0.1', 'TRIX_10_5', 'BBB_5_2.0_5',\n",
      "       'BBP_5_2.0_5', 'BBB_10_2.0_10', 'BBP_10_2.0_10', 'BBP_15_2.0_15',\n",
      "       'BBP_20_2.0_20', 'close_change', 'high_pct', 'close_change_roll5',\n",
      "       'RSI_14_roll5', 'ATR_14_roll5', 'volume_roll5', 'high_pct_roll5',\n",
      "       'volatility_5', 'price_ema5', 'volume_ema5', 'price_to_ema5',\n",
      "       'volume_change_roll5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Prepare your features and target\n",
    "X = df.drop(\"color_change\", axis=1)\n",
    "y = df[\"color_change\"]\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Logistic Regression CV ROC AUC score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Random Forest CV ROC AUC score: {np.mean(cv_scores)}\")\n",
    "\n",
    "print(\"\\n\", X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

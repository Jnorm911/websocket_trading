{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as pta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Convert 'color' column to binary representation\n",
    "    df[\"color\"] = df[\"color\"].map({\"red\": 0, \"green\": 1})\n",
    "\n",
    "    df[\"color_change\"] = df[\"color\"].diff().ne(0).astype(int)\n",
    "    df[\"color_change\"].fillna(0, inplace=True)\n",
    "\n",
    "    # Fill NaNs in specified columns with 0\n",
    "    df[\"PSARl_0.01_0.1\"].fillna(0, inplace=True)\n",
    "    df[\"PSARs_0.01_0.1\"].fillna(0, inplace=True)\n",
    "    df[\"ICS_15\"].fillna(0, inplace=True)\n",
    "\n",
    "    # # Diff features\n",
    "    # df[\"volume_diff\"] = df[\"volume\"].diff()\n",
    "    # df[\"turnover_diff\"] = df[\"turnover\"].diff()\n",
    "    # df[\"close_open\"] = df[\"close\"] - df[\"open\"]\n",
    "    # df[\"open_close\"] = df[\"open\"] - df[\"close\"]\n",
    "    # df[\"high_low\"] = df[\"high\"] - df[\"low\"]\n",
    "    df[\"close_change\"] = df[\"close\"].diff()\n",
    "    # df[\"open_change\"] = df[\"open\"].diff()\n",
    "    # df[\"h_diff\"] = df[\"high\"] - df[\"high\"].shift(1)\n",
    "    # df[\"l_diff\"] = df[\"low\"] - df[\"low\"].shift(1)\n",
    "    # df[\"hl_shift\"] = df[\"high\"] - df[\"low\"].shift(1)\n",
    "    df[\"high_pct\"] = df[\"high\"].pct_change()\n",
    "    # df[\"low_pct\"] = df[\"low\"].pct_change()\n",
    "    # df[\"close_open_lag\"] = df[\"close_open\"].shift(1)\n",
    "    # df[\"high_low_lag\"] = df[\"high_low\"].shift(1)\n",
    "\n",
    "    # # Ratio features\n",
    "    # N = 10\n",
    "    # df[\"PSARr_to_RVI\"] = df[\"PSARr_0.01_0.1\"] / df[\"RVI_5\"]\n",
    "    # df[\"High_low_pct_diff\"] = df[\"high_pct\"] - df[\"low_pct\"]\n",
    "    # consecutive_same_color = (\n",
    "    #     (df[\"color\"] == df[\"color\"].shift(1))\n",
    "    #     .astype(int)\n",
    "    #     .groupby(df[\"color\"].ne(df[\"color\"].shift()).cumsum())\n",
    "    #     .cumcount()\n",
    "    # )\n",
    "    # df[\"past_candle_color_ratio\"] = consecutive_same_color.rolling(N).sum() / N\n",
    "    # df[\"volume_to_avg_vol_ratio\"] = df[\"volume\"] / df[\"avg_vol_last_100\"]\n",
    "    # df[\"turnover_to_avg_turnover_ratio\"] = (\n",
    "    #     df[\"turnover\"] / df[\"turnover\"].rolling(N).mean()\n",
    "    # )\n",
    "    # upper_shadow = (df[\"high\"] - df[[\"open\", \"close\"]].max(axis=1)).apply(abs)\n",
    "    # lower_shadow = (df[\"low\"] - df[[\"open\", \"close\"]].min(axis=1)).apply(abs)\n",
    "    # df[\"shadow_ratio\"] = upper_shadow / lower_shadow\n",
    "    # df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # # Lagged features\n",
    "    # df[\"lag5_high_pct\"] = df[\"high_pct\"].shift(5)\n",
    "    # df[\"lag5_close_change\"] = df[\"close_change\"].shift(5)\n",
    "    # df[\"lag5_close_change\"] = df[\"close_change\"].shift(1)\n",
    "\n",
    "    # Rolling features\n",
    "    df[\"close_change_roll5\"] = df[\"close_change\"].rolling(20).mean()\n",
    "    df[\"RSI_14_roll5\"] = df[\"RSI_14\"].rolling(20).mean()\n",
    "    df[\"ATR_14_roll5\"] = df[\"ATR_14\"].rolling(20).mean()\n",
    "    df[\"volume_roll5\"] = df[\"volume\"].rolling(20).mean()\n",
    "    df[\"high_pct_roll5\"] = df[\"high_pct\"].rolling(20).mean()\n",
    "    df[\"volatility_5\"] = df[\"close\"].rolling(20).std()\n",
    "    df[\"price_ema5\"] = df[\"close\"].ewm(span=20).mean()\n",
    "    df[\"volume_ema5\"] = df[\"volume\"].ewm(span=20).mean()\n",
    "    df[\"price_to_ema5\"] = df[\"close\"] / df[\"price_ema5\"] - 1\n",
    "    df[\"volume_change_roll5\"] = df[\"volume\"].pct_change().rolling(20).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_data(df):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return df_scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set display options to show all rows and columns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Load the data\n",
    "data_path = (\n",
    "    \"../../../data/kc/btc/heiken_ashi/with_trade_indicators/raw/kc_btc_12min_ha_ti.csv\"\n",
    ")\n",
    "\n",
    "# List of features to drop\n",
    "features_to_drop = [\n",
    "    ########\n",
    "    # Univariate Feature Selection Features #\n",
    "    #######\n",
    "    \"cmf\",\n",
    "    \"MACDs_6_13_5_6_13_5\",\n",
    "    \"RVI_10\",\n",
    "    \"BBL_5_2.0_5\",\n",
    "    \"STOCHk_14_3_3\",\n",
    "    \"STOCHk_14_3_3_7_3_3\",\n",
    "    \"STOCHk_14_3_3_10_3_3\",\n",
    "    \"MACDh_6_13_5_6_13_5\",\n",
    "    \"MACDh_12_26_9\",\n",
    "    \"BBL_10_2.0_10\",\n",
    "    \"R3\",\n",
    "    \"R2\",\n",
    "    \"R1\",\n",
    "    \"open\",\n",
    "    \"BBL_15_2.0_15\",\n",
    "    \"SMA_5\",\n",
    "    \"BBM_5_2.0_5\",\n",
    "    \"PP\",\n",
    "    \"BBL_20_2.0_20\",\n",
    "    \"EMA_5\",\n",
    "    \"ITS_5\",\n",
    "    \"SMA_10\",\n",
    "    \"BBM_10_2.0_10\",\n",
    "    \"EMA_10\",\n",
    "    \"S1\",\n",
    "    \"SMA_20\",\n",
    "    \"EMA_2\",\n",
    "    \"BBM_15_2.0_15\",\n",
    "    \"S2\",\n",
    "    \"BBM_20_2.0_20\",\n",
    "    \"IKS_15\",\n",
    "    \"close\",\n",
    "    \"PSARaf_0.01_0.1\",\n",
    "    \"S3\",\n",
    "    \"MACD_6_13_5_6_13_5\",\n",
    "    \"ISA_5\",\n",
    "    \"ISB_15\",\n",
    "    \"BBU_15_2.0_15\",\n",
    "    \"BBU_10_2.0_10\",\n",
    "    \"BBB_15_2.0_15\",\n",
    "    \"obv\",\n",
    "    \"BBU_5_2.0_5\",\n",
    "    \"high\",\n",
    "    \"STOCHd_14_3_3\",\n",
    "    \"STOCHd_14_3_3_7_3_3\",\n",
    "    \"STOCHd_14_3_3_10_3_3\",\n",
    "    \"TRIXs_10_5\",\n",
    "    \"MACD_12_26_9\",\n",
    "    \"PSARs_0.01_0.1\",\n",
    "    \"TRIXs_18_9\",\n",
    "    \"ICS_15\",\n",
    "    \"ROC_10\",\n",
    "    \"BBB_20_2.0_20\",\n",
    "    \"bollinger_bandwidth\",\n",
    "    \"RVI_15\",\n",
    "    \"MACDs_12_26_9\",\n",
    "    \"PSARl_0.01_0.1\",\n",
    "    \"TRIX_18_9\",\n",
    "    \"TRIXs_12_6\",\n",
    "    \"ROC_5\",\n",
    "    \"BBU_20_2.0_20\",\n",
    "    \"low\",\n",
    "    \"TRIX_12_6\",\n",
    "    \"ROC_14\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(data_path)\n",
    "\n",
    "# Preprocess the data\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# Drop the specified features\n",
    "df = df.drop(columns=features_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Prepare TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Fill\n",
    "df.ffill(inplace=True)\n",
    "\n",
    "# Backward Fill\n",
    "df.bfill(inplace=True)\n",
    "\n",
    "# Re-scale the data to include the new feature\n",
    "df_scaled = scale_data(df)\n",
    "\n",
    "# print(df.isna().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define initial features for ablation process\n",
    "# ablation_features = [\n",
    "#     \"cmf\",\n",
    "#     \"MACDs_6_13_5_6_13_5\",\n",
    "#     \"RVI_10\",\n",
    "#     \"BBL_5_2.0_5\",\n",
    "#     \"STOCHk_14_3_3\",\n",
    "#     \"STOCHk_14_3_3_7_3_3\",\n",
    "#     \"STOCHk_14_3_3_10_3_3\",\n",
    "#     \"MACDh_6_13_5_6_13_5\",\n",
    "#     \"MACDh_12_26_9\",\n",
    "#     \"BBL_10_2.0_10\",\n",
    "#     \"R3\",\n",
    "#     \"R2\",\n",
    "#     \"o_shift\",\n",
    "#     \"R1\",\n",
    "#     \"open\",\n",
    "#     \"BBL_15_2.0_15\",\n",
    "#     \"SMA_5\",\n",
    "#     \"BBM_5_2.0_5\",\n",
    "#     \"c_shift\",\n",
    "#     \"PP\",\n",
    "#     \"BBL_20_2.0_20\",\n",
    "#     \"EMA_5\",\n",
    "#     \"ITS_5\",\n",
    "#     \"SMA_10\",\n",
    "#     \"BBM_10_2.0_10\",\n",
    "#     \"EMA_10\",\n",
    "#     \"S1\",\n",
    "#     \"SMA_20\",\n",
    "#     \"EMA_2\",\n",
    "#     \"BBM_15_2.0_15\",\n",
    "#     \"S2\",\n",
    "#     \"BBM_20_2.0_20\",\n",
    "#     \"IKS_15\",\n",
    "#     \"close\",\n",
    "#     \"PSARaf_0.01_0.1\",\n",
    "#     \"S3\",\n",
    "#     \"MACD_6_13_5_6_13_5\",\n",
    "#     \"ISA_5\",\n",
    "#     \"ISB_15\",\n",
    "#     \"BBU_15_2.0_15\",\n",
    "#     \"BBU_10_2.0_10\",\n",
    "#     \"BBB_15_2.0_15\",\n",
    "#     \"obv\",\n",
    "#     \"BBU_5_2.0_5\",\n",
    "#     \"high\",\n",
    "#     \"STOCHd_14_3_3\",\n",
    "#     \"STOCHd_14_3_3_7_3_3\",\n",
    "#     \"STOCHd_14_3_3_10_3_3\",\n",
    "#     \"TRIXs_10_5\",\n",
    "#     \"MACD_12_26_9\",\n",
    "#     \"PSARs_0.01_0.1\",\n",
    "#     \"TRIXs_18_9\",\n",
    "#     \"ICS_15\",\n",
    "#     \"ROC_10\",\n",
    "#     \"BBB_20_2.0_20\",\n",
    "#     \"bollinger_bandwidth\",\n",
    "#     \"RVI_15\",\n",
    "#     \"MACDs_12_26_9\",\n",
    "#     \"PSARl_0.01_0.1\",\n",
    "#     \"TRIX_18_9\",\n",
    "#     \"volume_diff_lag\",\n",
    "#     \"TRIXs_12_6\",\n",
    "#     \"ROC_5\",\n",
    "#     \"BBU_20_2.0_20\",\n",
    "#     \"low\",\n",
    "#     \"TRIX_12_6\",\n",
    "#     \"ROC_14\",\n",
    "# ]\n",
    "\n",
    "# # All the features in the data\n",
    "# all_features = list(df.columns)\n",
    "# all_features.remove(\"color_change\")  # we remove the target variable\n",
    "\n",
    "# log_reg_scores = {}\n",
    "# rf_scores = {}\n",
    "\n",
    "# for feature in ablation_features:\n",
    "#     # Remove one feature from the all features list\n",
    "#     current_features = [f for f in all_features if f != feature]\n",
    "\n",
    "#     # Prepare your features and target\n",
    "#     X = df[current_features]\n",
    "#     y = df[\"color_change\"]\n",
    "\n",
    "#     # Logistic Regression\n",
    "#     log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(log_reg, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "#     log_reg_scores[feature] = np.mean(cv_scores)\n",
    "\n",
    "# # Print logistic regression scores sorted by score\n",
    "# print(\"Logistic Regression Scores\")\n",
    "# for feature, score in sorted(\n",
    "#     log_reg_scores.items(), key=lambda item: item[1], reverse=True\n",
    "# ):\n",
    "#     print(f\"CV ROC AUC score without {feature}: {score}\")\n",
    "\n",
    "# for feature in ablation_features:\n",
    "#     # Remove one feature from the all features list\n",
    "#     current_features = [f for f in all_features if f != feature]\n",
    "\n",
    "#     # Prepare your features and target\n",
    "#     X = df[current_features]\n",
    "#     y = df[\"color_change\"]\n",
    "\n",
    "#     # Random Forest\n",
    "#     rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(rf, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "#     rf_scores[feature] = np.mean(cv_scores)\n",
    "\n",
    "# # Print random forest scores sorted by score\n",
    "# print(\"\\nRandom Forest Scores\")\n",
    "# for feature, score in sorted(rf_scores.items(), key=lambda item: item[1], reverse=True):\n",
    "#     print(f\"CV ROC AUC score without {feature}: {score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Ablation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define initial features for ablation process\n",
    "# ablation_features = [\n",
    "#     \"cmf\",\n",
    "#     \"MACDs_6_13_5_6_13_5\",\n",
    "#     \"RVI_10\",\n",
    "#     \"BBL_5_2.0_5\",\n",
    "#     \"STOCHk_14_3_3\",\n",
    "#     \"STOCHk_14_3_3_7_3_3\",\n",
    "#     \"STOCHk_14_3_3_10_3_3\",\n",
    "#     \"MACDh_6_13_5_6_13_5\",\n",
    "#     \"MACDh_12_26_9\",\n",
    "#     \"BBL_10_2.0_10\",\n",
    "#     \"R3\",\n",
    "#     \"R2\",\n",
    "#     \"o_shift\",\n",
    "#     \"R1\",\n",
    "#     \"open\",\n",
    "#     \"BBL_15_2.0_15\",\n",
    "#     \"SMA_5\",\n",
    "#     \"BBM_5_2.0_5\",\n",
    "#     \"c_shift\",\n",
    "#     \"PP\",\n",
    "#     \"BBL_20_2.0_20\",\n",
    "#     \"EMA_5\",\n",
    "#     \"ITS_5\",\n",
    "#     \"SMA_10\",\n",
    "#     \"BBM_10_2.0_10\",\n",
    "#     \"EMA_10\",\n",
    "#     \"S1\",\n",
    "#     \"SMA_20\",\n",
    "#     \"EMA_2\",\n",
    "#     \"BBM_15_2.0_15\",\n",
    "#     \"S2\",\n",
    "#     \"BBM_20_2.0_20\",\n",
    "#     \"IKS_15\",\n",
    "#     \"close\",\n",
    "#     \"PSARaf_0.01_0.1\",\n",
    "#     \"S3\",\n",
    "#     \"MACD_6_13_5_6_13_5\",\n",
    "#     \"ISA_5\",\n",
    "#     \"ISB_15\",\n",
    "#     \"BBU_15_2.0_15\",\n",
    "#     \"BBU_10_2.0_10\",\n",
    "#     \"BBB_15_2.0_15\",\n",
    "#     \"obv\",\n",
    "#     \"BBU_5_2.0_5\",\n",
    "#     \"high\",\n",
    "#     \"STOCHd_14_3_3\",\n",
    "#     \"STOCHd_14_3_3_7_3_3\",\n",
    "#     \"STOCHd_14_3_3_10_3_3\",\n",
    "#     \"TRIXs_10_5\",\n",
    "#     \"MACD_12_26_9\",\n",
    "#     \"PSARs_0.01_0.1\",\n",
    "#     \"TRIXs_18_9\",\n",
    "#     \"ICS_15\",\n",
    "#     \"ROC_10\",\n",
    "#     \"BBB_20_2.0_20\",\n",
    "#     \"bollinger_bandwidth\",\n",
    "#     \"RVI_15\",\n",
    "#     \"MACDs_12_26_9\",\n",
    "#     \"PSARl_0.01_0.1\",\n",
    "#     \"TRIX_18_9\",\n",
    "#     \"volume_diff_lag\",\n",
    "#     \"TRIXs_12_6\",\n",
    "#     \"ROC_5\",\n",
    "#     \"BBU_20_2.0_20\",\n",
    "#     \"low\",\n",
    "#     \"TRIX_12_6\",\n",
    "#     \"ROC_14\",\n",
    "# ]\n",
    "\n",
    "# # All the features in the data\n",
    "# all_features = list(df.columns)\n",
    "# all_features.remove(\"color_change\")  # we remove the target variable\n",
    "\n",
    "# # Base features are all features excluding the ones in the ablation list\n",
    "# base_features = [\n",
    "#     feature for feature in all_features if feature not in ablation_features\n",
    "# ]\n",
    "\n",
    "# log_reg_scores = {}\n",
    "# rf_scores = {}\n",
    "\n",
    "# for feature in ablation_features:\n",
    "#     # Add one feature from the ablation list\n",
    "#     current_features = base_features + [feature]\n",
    "\n",
    "#     # Prepare your features and target\n",
    "#     X = df[current_features]\n",
    "#     y = df[\"color_change\"]\n",
    "\n",
    "#     # Logistic Regression\n",
    "#     log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(log_reg, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "#     log_reg_scores[feature] = np.mean(cv_scores)\n",
    "\n",
    "# # Print logistic regression scores sorted by score\n",
    "# print(\"Logistic Regression Scores\")\n",
    "# for feature, score in sorted(\n",
    "#     log_reg_scores.items(), key=lambda item: item[1], reverse=True\n",
    "# ):\n",
    "#     print(f\"CV ROC AUC score with {feature}: {score}\")\n",
    "\n",
    "# for feature in ablation_features:\n",
    "#     # Add one feature from the ablation list\n",
    "#     current_features = base_features + [feature]\n",
    "\n",
    "#     # Prepare your features and target\n",
    "#     X = df[current_features]\n",
    "#     y = df[\"color_change\"]\n",
    "\n",
    "#     # Random Forest\n",
    "#     rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "#     # Cross-validation\n",
    "#     cv_scores = cross_val_score(rf, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "#     rf_scores[feature] = np.mean(cv_scores)\n",
    "\n",
    "# # Print random forest scores sorted by score\n",
    "# print(\"\\nRandom Forest Scores\")\n",
    "# for feature, score in sorted(rf_scores.items(), key=lambda item: item[1], reverse=True):\n",
    "#     print(f\"CV ROC AUC score with {feature}: {score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(\"color_change\", axis=1)\n",
    "# y = df[\"color_change\"]\n",
    "\n",
    "# # Re-scale the data to include the new feature\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Fit XGBoost model and get feature importances\n",
    "# xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "# xgb.fit(X_scaled, y)\n",
    "\n",
    "# # Store the feature importances in a pandas series, then sort it in descending order\n",
    "# importances = pd.Series(xgb.feature_importances_, index=X.columns)\n",
    "# importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# print(\"Feature importances:\")\n",
    "# print(importances_sorted)\n",
    "\n",
    "# # Now we can apply Logistic Regression and Random Forests using the features\n",
    "# # Logistic Regression\n",
    "# log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# # Cross-validation\n",
    "# cv_scores = cross_val_score(log_reg, X_scaled, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "# print(f\"\\nLogistic Regression CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# # Random Forest\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Cross-validation\n",
    "# cv_scores = cross_val_score(rf, X_scaled, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "# print(f\"Random Forest CV F1 score: {np.mean(cv_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop(\"color_change\", axis=1)\n",
    "# y = df[\"color_change\"]\n",
    "\n",
    "# # Re-scale the data to include the new feature\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Recursive Feature Elimination\n",
    "# # Here we use Logistic Regression as the model to evaluate the feature importance\n",
    "# # You can replace it with any model you prefer\n",
    "# model = LogisticRegression(random_state=42, max_iter=500)\n",
    "# rfe = RFECV(model)\n",
    "# rfe.fit(X_scaled, y)\n",
    "\n",
    "# # Get the features ranking\n",
    "# feature_ranking = {\n",
    "#     feature_name: rank for feature_name, rank in zip(X.columns, rfe.ranking_)\n",
    "# }\n",
    "\n",
    "# # Sort and print the feature ranking\n",
    "# for feature_name, rank in sorted(feature_ranking.items(), key=lambda item: item[1]):\n",
    "#     print(f\"{feature_name}: {rank}\")\n",
    "\n",
    "# # Transform X to include only the selected features\n",
    "# X_transformed = rfe.transform(X_scaled)\n",
    "\n",
    "# # Logistic Regression with transformed features\n",
    "# log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# # Cross-validation\n",
    "# cv_scores = cross_val_score(log_reg, X_transformed, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "# print(f\"\\nLogistic Regression CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# # Random Forest with transformed features\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Cross-validation\n",
    "# cv_scores = cross_val_score(rf, X_transformed, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "# print(f\"Random Forest CV F1 score: {np.mean(cv_scores)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection Process ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_pct: 110.94480524940894\n",
      "volume: 91.92131389069183\n",
      "turnover: 89.7339138964675\n",
      "BBB_5_2.0_5: 40.51976375637065\n",
      "PSARr_0.01_0.1: 33.222447998121424\n",
      "ATR_5: 8.09442137040173\n",
      "volume_ema5: 7.443922940784361\n",
      "BBP_15_2.0_15: 6.265493240326674\n",
      "CCI_14: 6.182127667059803\n",
      "CCI_10: 5.536539624921639\n",
      "close_change: 5.264615289857336\n",
      "BBP_10_2.0_10: 4.8425351451218335\n",
      "volume_change_roll5: 4.467484190456277\n",
      "BBP_20_2.0_20: 4.391442577137113\n",
      "BBB_10_2.0_10: 3.449706673320239\n",
      "volume_roll5: 3.405378790575545\n",
      "ATR_10: 3.3469011580871424\n",
      "mfi: 2.7913408994283717\n",
      "color: 2.650632704450662\n",
      "ATR_14: 2.5343458390778144\n",
      "BBP_5_2.0_5: 2.5087912415826117\n",
      "CCI_5: 2.4255515533899725\n",
      "avg_vol_last_100: 2.179824307137989\n",
      "RSI_5: 1.8759432547309074\n",
      "RVI_5: 1.770072600613789\n",
      "RSI_10: 1.700735361160013\n",
      "RSI_14: 1.3811647003530192\n",
      "time: 1.285269957412531\n",
      "high_pct_roll5: 1.2662544938393032\n",
      "ATR_14_roll5: 0.8521782525282505\n",
      "price_ema5: 0.31805785232331435\n",
      "TRIX_10_5: 0.31578147717879135\n",
      "RSI_14_roll5: 0.3015932283688058\n",
      "close_change_roll5: 0.12167940381357349\n",
      "price_to_ema5: 0.10717434509139667\n",
      "volatility_5: 0.10047315367559004\n",
      "\n",
      "Logistic Regression CV F1 score: 0.6692820572508955\n",
      "Random Forest CV F1 score: 0.8100051138958743\n",
      "\n",
      " Index(['time', 'volume', 'turnover', 'color', 'avg_vol_last_100', 'RSI_5',\n",
      "       'RSI_10', 'RSI_14', 'ATR_14', 'ATR_10', 'ATR_5', 'CCI_14', 'CCI_10',\n",
      "       'CCI_5', 'mfi', 'RVI_5', 'PSARr_0.01_0.1', 'TRIX_10_5', 'BBB_5_2.0_5',\n",
      "       'BBP_5_2.0_5', 'BBB_10_2.0_10', 'BBP_10_2.0_10', 'BBP_15_2.0_15',\n",
      "       'BBP_20_2.0_20', 'close_change', 'high_pct', 'close_change_roll5',\n",
      "       'RSI_14_roll5', 'ATR_14_roll5', 'volume_roll5', 'high_pct_roll5',\n",
      "       'volatility_5', 'price_ema5', 'volume_ema5', 'price_to_ema5',\n",
      "       'volume_change_roll5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(\"color_change\", axis=1)\n",
    "y = df[\"color_change\"]\n",
    "\n",
    "# Re-scale the data to include the new feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "selector.fit(X_scaled, y)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "features_df_new = X.iloc[:, cols]\n",
    "\n",
    "# Store the scores of each feature in a dictionary\n",
    "feature_scores = {\n",
    "    feature_name: score for feature_name, score in zip(X.columns, selector.scores_)\n",
    "}\n",
    "\n",
    "# Sort the dictionary by value in descending order and print the scores\n",
    "for feature_name, score in sorted(\n",
    "    feature_scores.items(), key=lambda item: item[1], reverse=True\n",
    "):\n",
    "    print(f\"{feature_name}: {score}\")\n",
    "\n",
    "# Now we can apply Logistic Regression and Random Forests on the new features_df_new\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, features_df_new, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "print(f\"\\nLogistic Regression CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, features_df_new, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "print(f\"Random Forest CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "print(\"\\n\", features_df_new.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression CV ROC AUC score: 0.5547196931554365\n",
      "Random Forest CV ROC AUC score: 0.8702600316865063\n",
      "\n",
      " Index(['time', 'volume', 'turnover', 'color', 'avg_vol_last_100', 'RSI_5',\n",
      "       'RSI_10', 'RSI_14', 'ATR_14', 'ATR_10', 'ATR_5', 'CCI_14', 'CCI_10',\n",
      "       'CCI_5', 'mfi', 'RVI_5', 'PSARr_0.01_0.1', 'TRIX_10_5', 'BBB_5_2.0_5',\n",
      "       'BBP_5_2.0_5', 'BBB_10_2.0_10', 'BBP_10_2.0_10', 'BBP_15_2.0_15',\n",
      "       'BBP_20_2.0_20', 'close_change', 'high_pct', 'close_change_roll5',\n",
      "       'RSI_14_roll5', 'ATR_14_roll5', 'volume_roll5', 'high_pct_roll5',\n",
      "       'volatility_5', 'price_ema5', 'volume_ema5', 'price_to_ema5',\n",
      "       'volume_change_roll5'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Prepare your features and target\n",
    "X = df.drop(\"color_change\", axis=1)\n",
    "y = df[\"color_change\"]\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Logistic Regression CV ROC AUC score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Random Forest CV ROC AUC score: {np.mean(cv_scores)}\")\n",
    "\n",
    "print(\"\\n\", X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

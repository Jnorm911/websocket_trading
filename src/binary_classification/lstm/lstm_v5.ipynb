{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import talib\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df[\"color_change\"] = df[\"color\"].diff().ne(0).astype(int)\n",
    "    df[\"color_change\"].fillna(0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_data(df):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "def add_heiken_ashi_features(df):\n",
    "    # Create Heiken Ashi DataFrame\n",
    "    ha_df = df.ta.ha()\n",
    "\n",
    "    # Rename the HA columns\n",
    "    ha_df.columns = [f\"HA_{col}\" for col in ha_df.columns]\n",
    "\n",
    "    # Join the HA columns to the original dataframe\n",
    "    df = df.join(ha_df)\n",
    "\n",
    "    # Heiken Ashi Close to Open\n",
    "    df[\"HA_close_open\"] = df[\"HA_close\"] - df[\"HA_open\"]\n",
    "\n",
    "    # Heiken Ashi High Low Range\n",
    "    df[\"HA_high_low\"] = df[\"HA_high\"] - df[\"HA_low\"]\n",
    "\n",
    "    # Heiken Ashi Body Range\n",
    "    df[\"HA_body\"] = abs(df[\"HA_close\"] - df[\"HA_open\"])\n",
    "\n",
    "    # Heiken Ashi Price Direction\n",
    "    df[\"HA_direction\"] = (df[\"HA_close\"] > df[\"HA_open\"]).astype(int)\n",
    "\n",
    "    # Heiken Ashi Volume-weighted Price\n",
    "    df[\"HA_vwap\"] = (df[\"HA_close\"] * df[\"volume\"]).cumsum() / df[\"volume\"].cumsum()\n",
    "\n",
    "    # Lag 1 feature\n",
    "    df[\"HA_close_lag1\"] = df[\"HA_close\"].shift(1)\n",
    "\n",
    "    # Close Change\n",
    "    df[\"HA_close_change\"] = df[\"HA_close\"].diff()\n",
    "\n",
    "    # Close % Change\n",
    "    df[\"HA_close_pct_change\"] = df[\"HA_close\"].pct_change()\n",
    "\n",
    "    # 5-period Simple Moving Average\n",
    "    df[\"HA_sma5\"] = df[\"HA_close\"].rolling(5).mean()\n",
    "\n",
    "    # 5-period Exponential Moving Average\n",
    "    df[\"HA_ema5\"] = df[\"HA_close\"].ewm(span=5).mean()\n",
    "\n",
    "    # Additional features\n",
    "    df[\"HA_ema10\"] = df[\"HA_close\"].ewm(span=10).mean()\n",
    "    df[\"HA_ema15\"] = df[\"HA_close\"].ewm(span=15).mean()\n",
    "    df[\"HA_pct_diff_ema5_15\"] = (\n",
    "        (df[\"HA_ema5\"] - df[\"HA_ema15\"]) / df[\"HA_ema15\"]\n",
    "    ) * 100\n",
    "    df[\"HA_rsi\"] = ta.rsi(df[\"HA_close\"])\n",
    "    # Calculate Short Term Exponential Moving Average\n",
    "    df[\"short_ema\"] = df[\"HA_close\"].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "    # Calculate Long Term Exponential Moving Average\n",
    "    df[\"long_ema\"] = df[\"HA_close\"].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # Calculate Moving Average Convergence Divergence (MACD)\n",
    "    df[\"HA_macd\"] = df[\"short_ema\"] - df[\"long_ema\"]\n",
    "\n",
    "    # Calculate Signal Line\n",
    "    df[\"HA_macds\"] = df[\"HA_macd\"].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # Calculate MACD Histogram\n",
    "    df[\"HA_macdh\"] = df[\"HA_macd\"] - df[\"HA_macds\"]\n",
    "\n",
    "    # Drop temporary short_ema and long_ema columns\n",
    "    df = df.drop([\"short_ema\", \"long_ema\"], axis=1)\n",
    "\n",
    "    df[\"HA_cci\"] = ta.cci(df[\"HA_high\"], df[\"HA_low\"], df[\"HA_close\"])\n",
    "    df[\"HA_atr\"] = ta.atr(df[\"HA_high\"], df[\"HA_low\"], df[\"HA_close\"])\n",
    "    df[\"HA_ha_close_bbp50_std\"] = (\n",
    "        ta.stdev(df[\"HA_close\"], 50) / df[\"HA_close\"]\n",
    "    )  # Bollinger Bands normalized width\n",
    "    df[\"HA_mfi\"] = ta.mfi(df[\"HA_high\"], df[\"HA_low\"], df[\"HA_close\"], df[\"volume\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_renko(df, timeperiod=14, multiplier=0.25):\n",
    "    # Calculate ATR\n",
    "    atr_values = talib.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod)\n",
    "\n",
    "    # Drop rows with NaN ATR values\n",
    "    df = df[atr_values.notna()]\n",
    "    atr_values = atr_values.dropna()\n",
    "\n",
    "    # Compute average ATR\n",
    "    average_atr = atr_values.mean()\n",
    "\n",
    "    # Set brick size\n",
    "    brick_size = average_atr * multiplier\n",
    "\n",
    "    renko_df = pd.DataFrame(\n",
    "        index=df.index, columns=[\"open\", \"high\", \"low\", \"close\", \"time_unix\"]\n",
    "    )\n",
    "\n",
    "    current_price = df[\"close\"][0]\n",
    "    last_reset_price = current_price\n",
    "\n",
    "    for idx in df.index[1:]:\n",
    "        current_price = df.loc[idx, \"close\"]\n",
    "        renko_df.loc[idx, \"time_unix\"] = df.loc[\n",
    "            idx, \"time_unix\"\n",
    "        ]  # Copy 'time_unix' from df\n",
    "\n",
    "        while last_reset_price + brick_size <= current_price:\n",
    "            renko_df.loc[idx] = [\n",
    "                last_reset_price,\n",
    "                last_reset_price + brick_size,\n",
    "                last_reset_price,\n",
    "                last_reset_price + brick_size,\n",
    "                df.loc[idx, \"time_unix\"],\n",
    "            ]\n",
    "            last_reset_price += brick_size\n",
    "\n",
    "        while last_reset_price - brick_size >= current_price:\n",
    "            renko_df.loc[idx] = [\n",
    "                last_reset_price,\n",
    "                last_reset_price - brick_size,\n",
    "                last_reset_price - brick_size,\n",
    "                last_reset_price,\n",
    "                df.loc[idx, \"time_unix\"],\n",
    "            ]\n",
    "            last_reset_price -= brick_size\n",
    "\n",
    "    return renko_df.dropna()\n",
    "\n",
    "\n",
    "def add_renko_features(df, renko_df):\n",
    "    # Append \"_renko\" to the column names of renko_df\n",
    "    renko_df.columns = [str(col) + \"_renko\" for col in renko_df.columns]\n",
    "\n",
    "    # Add EMA, SMA, WMA, RSI, and Bollinger Bands to renko_df\n",
    "    renko_df[\"sma_renko\"] = talib.SMA(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    renko_df[\"ema_renko\"] = talib.EMA(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    renko_df[\"wma_renko\"] = talib.WMA(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    renko_df[\"rsi_renko\"] = talib.RSI(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    (\n",
    "        renko_df[\"upper_band_renko\"],\n",
    "        renko_df[\"middle_band_renko\"],\n",
    "        renko_df[\"lower_band_renko\"],\n",
    "    ) = talib.BBANDS(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    # Add ROC and Momentum to renko_df\n",
    "    renko_df[\"roc_renko\"] = talib.ROC(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    renko_df[\"momentum_renko\"] = renko_df[\"close_renko\"].diff()\n",
    "\n",
    "    # Add Stochastic Oscillator to renko_df\n",
    "    renko_df[\"k_renko\"], renko_df[\"d_renko\"] = talib.STOCH(\n",
    "        renko_df[\"high_renko\"],\n",
    "        renko_df[\"low_renko\"],\n",
    "        renko_df[\"close_renko\"],\n",
    "        fastk_period=5,\n",
    "        slowk_period=3,\n",
    "        slowd_period=3,\n",
    "    )\n",
    "    df = df.join(\n",
    "        renko_df.set_index(\"time_unix_renko\"), on=\"time_unix\"\n",
    "    )  # Join Renko data with the original DataFrame\n",
    "\n",
    "    # Derived features\n",
    "    df[\"close_open_renko\"] = df[\"close_renko\"] - df[\"open_renko\"]\n",
    "    df[\"high_low_renko\"] = df[\"high_renko\"] - df[\"low_renko\"]\n",
    "    df[\"close_change_renko\"] = df[\"close_renko\"].diff()\n",
    "    df[\"direction_renko\"] = df[\"close_change_renko\"].apply(lambda x: int(x > 0))\n",
    "    df[\"direction_change\"] = df[\"direction_renko\"].diff().abs()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def kagi(df):\n",
    "    kagi_data = []\n",
    "    cur_col = {\n",
    "        \"kagi_dir\": \"up\",\n",
    "        \"kagi_start\": df.loc[df.index[0], \"close\"],\n",
    "        \"time_unix\": df.loc[df.index[0], \"time_unix\"],\n",
    "    }\n",
    "    for index, row in df.iterrows():\n",
    "        cp = row[\"close\"]\n",
    "        diff = cp - cur_col[\"kagi_start\"]\n",
    "        if cur_col[\"kagi_dir\"] == \"up\":\n",
    "            if diff < 0:\n",
    "                kagi_data.append(cur_col.copy())\n",
    "                cur_col[\"kagi_dir\"] = \"down\"\n",
    "        else:\n",
    "            if diff > 0:\n",
    "                kagi_data.append(cur_col.copy())\n",
    "                cur_col[\"kagi_dir\"] = \"up\"\n",
    "        cur_col[\"kagi_start\"] = cp\n",
    "        cur_col[\"time_unix\"] = row[\"time_unix\"]\n",
    "    kagi_data.append(cur_col)\n",
    "    kagi_df = pd.DataFrame(kagi_data)\n",
    "    return kagi_df\n",
    "\n",
    "\n",
    "def add_kagi_features(df, kagi_df):\n",
    "    # Calculate simple moving averages\n",
    "    kagi_df[\"kagi_sma5\"] = talib.SMA(kagi_df[\"kagi_start\"], timeperiod=5)\n",
    "    kagi_df[\"kagi_sma10\"] = talib.SMA(kagi_df[\"kagi_start\"], timeperiod=10)\n",
    "    kagi_df[\"kagi_sma20\"] = talib.SMA(kagi_df[\"kagi_start\"], timeperiod=20)\n",
    "\n",
    "    # Calculate Bollinger Bands\n",
    "    kagi_df[\"kagi_upper\"], kagi_df[\"kagi_middle\"], kagi_df[\"kagi_lower\"] = talib.BBANDS(\n",
    "        kagi_df[\"kagi_start\"], timeperiod=5, nbdevup=2, nbdevdn=2, matype=0\n",
    "    )\n",
    "\n",
    "    # Calculate MACD\n",
    "    (\n",
    "        kagi_df[\"kagi_macd\"],\n",
    "        kagi_df[\"kagi_macdsignal\"],\n",
    "        kagi_df[\"kagi_macdhist\"],\n",
    "    ) = talib.MACD(kagi_df[\"kagi_start\"], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "    # Calculate RSI\n",
    "    kagi_df[\"kagi_rsi\"] = talib.RSI(kagi_df[\"kagi_start\"], timeperiod=14)\n",
    "\n",
    "    # Calculate Stochastic Oscillator\n",
    "    kagi_df[\"kagi_slowk\"], kagi_df[\"kagi_slowd\"] = talib.STOCH(\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        5,\n",
    "        3,\n",
    "        0,\n",
    "        3,\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    kagi_df[\"kagi_atr\"] = talib.ATR(\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        timeperiod=14,\n",
    "    )\n",
    "\n",
    "    kagi_df[\"kagi_adx\"] = talib.ADX(\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        timeperiod=14,\n",
    "    )\n",
    "\n",
    "    # Convert df.index to Unix timestamp\n",
    "    df.index = df.index.astype(np.int64) // 10**9\n",
    "\n",
    "    # Align the volume series with kagi_df\n",
    "    aligned_volume = df[\"volume\"].reindex(kagi_df[\"time_unix\"], method=\"pad\")\n",
    "\n",
    "    # Calculate Accumulation / Distribution Line\n",
    "    kagi_df[\"kagi_ad\"] = talib.AD(\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        aligned_volume,\n",
    "    )\n",
    "\n",
    "    return kagi_df\n",
    "\n",
    "\n",
    "def three_line_break(df):\n",
    "    df_tlb = pd.DataFrame()\n",
    "    df_tlb[\"open\"] = df[\"open\"].reset_index(drop=True)\n",
    "    df_tlb[\"close\"] = df[\"close\"].reset_index(drop=True)\n",
    "    df_tlb[\"high\"] = df[\"high\"].reset_index(drop=True)\n",
    "    df_tlb[\"low\"] = df[\"low\"].reset_index(drop=True)\n",
    "    df_tlb[\"volume\"] = df[\"volume\"].reset_index(drop=True)\n",
    "    df_tlb[\"tlb_direction\"] = None\n",
    "    df_tlb[\"time_unix\"] = df[\n",
    "        \"time_unix\"\n",
    "    ].values  # Copy 'time_unix' from original dataframe\n",
    "\n",
    "    if df_tlb[\"close\"][1] > df_tlb[\"close\"][0]:\n",
    "        df_tlb.loc[1, \"tlb_direction\"] = \"up\"\n",
    "    else:\n",
    "        df_tlb.loc[1, \"tlb_direction\"] = \"down\"\n",
    "\n",
    "    for i in range(2, len(df_tlb)):\n",
    "        up_condition = (df_tlb[\"close\"][i] > df_tlb[\"close\"][i - 3 : i].max()) and (\n",
    "            df_tlb[\"tlb_direction\"][i - 1] == \"down\"\n",
    "        )\n",
    "        down_condition = (df_tlb[\"close\"][i] < df_tlb[\"close\"][i - 3 : i].min()) and (\n",
    "            df_tlb[\"tlb_direction\"][i - 1] == \"up\"\n",
    "        )\n",
    "\n",
    "        if up_condition:\n",
    "            df_tlb.loc[i, \"tlb_direction\"] = \"up\"\n",
    "        elif down_condition:\n",
    "            df_tlb.loc[i, \"tlb_direction\"] = \"down\"\n",
    "        else:\n",
    "            df_tlb.loc[i, \"tlb_direction\"] = df_tlb.loc[i - 1, \"tlb_direction\"]\n",
    "\n",
    "    return df_tlb\n",
    "\n",
    "\n",
    "def add_tlb_features(df):\n",
    "    # Add Relative Strength Index (RSI) of the close price\n",
    "    df[\"tlb_RSI\"] = talib.RSI(df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Moving Average Convergence Divergence (MACD) of the close price\n",
    "    df[\"tlb_MACD\"], df[\"tlb_MACD_signal\"], df[\"tlb_MACD_hist\"] = talib.MACD(\n",
    "        df[\"close\"], fastperiod=12, slowperiod=26, signalperiod=9\n",
    "    )\n",
    "\n",
    "    # Add Bollinger Bands of the close price\n",
    "    df[\"tlb_upper_band\"], df[\"tlb_middle_band\"], df[\"tlb_lower_band\"] = talib.BBANDS(\n",
    "        df[\"close\"], timeperiod=20\n",
    "    )\n",
    "\n",
    "    # Add Simple Moving Average (SMA) of the close price\n",
    "    df[\"tlb_SMA\"] = talib.SMA(df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Exponential Moving Average (EMA) of the close price\n",
    "    df[\"tlb_EMA\"] = talib.EMA(df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Rate of Change (ROC) of the close price\n",
    "    df[\"tlb_ROC\"] = talib.ROC(df[\"close\"], timeperiod=10)\n",
    "\n",
    "    # Add Average True Range (ATR) of the close price\n",
    "    df[\"tlb_ATR\"] = talib.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Momentum of the close price\n",
    "    df[\"tlb_momentum\"] = talib.MOM(df[\"close\"], timeperiod=10)\n",
    "\n",
    "    # Add Stochastic Oscillator %K and %D of the close price\n",
    "    df[\"tlb_slowk\"], df[\"tlb_slowd\"] = talib.STOCH(\n",
    "        df[\"high\"],\n",
    "        df[\"low\"],\n",
    "        df[\"close\"],\n",
    "        fastk_period=5,\n",
    "        slowk_period=3,\n",
    "        slowk_matype=0,\n",
    "        slowd_period=3,\n",
    "        slowd_matype=0,\n",
    "    )\n",
    "\n",
    "    # Add Commodity Channel Index (CCI) of the close price\n",
    "    df[\"tlb_CCI\"] = talib.CCI(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add On Balance Volume (OBV)\n",
    "    df[\"tlb_OBV\"] = talib.OBV(df[\"close\"], df[\"volume\"])\n",
    "\n",
    "    # Add Moving Average of High, Low, Open prices\n",
    "    df[\"tlb_MA_high\"] = talib.MA(df[\"high\"], timeperiod=14)\n",
    "    df[\"tlb_MA_low\"] = talib.MA(df[\"low\"], timeperiod=14)\n",
    "    df[\"tlb_MA_open\"] = talib.MA(df[\"open\"], timeperiod=14)\n",
    "\n",
    "    # Add Historical volatility\n",
    "    df[\"tlb_volatility\"] = (\n",
    "        talib.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14) / df[\"tlb_SMA\"]\n",
    "    )\n",
    "\n",
    "    # Add Money Flow Index (MFI)\n",
    "    df[\"tlb_MFI\"] = talib.MFI(\n",
    "        df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"], timeperiod=14\n",
    "    )\n",
    "\n",
    "    # Add Chaikin Money Flow (CMF)\n",
    "    df[\"tlb_CMF\"] = (\n",
    "        df[\"close\"] - df[\"low\"] - (df[\"high\"] - df[\"close\"]) / (df[\"high\"] - df[\"low\"])\n",
    "    ) * df[\"volume\"]\n",
    "\n",
    "    # Add Williamâ€™s %R\n",
    "    df[\"tlb_WilliamsR\"] = talib.WILLR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Ultimate Oscillator\n",
    "    df[\"tlb_UO\"] = talib.ULTOSC(\n",
    "        df[\"high\"],\n",
    "        df[\"low\"],\n",
    "        df[\"close\"],\n",
    "        timeperiod1=7,\n",
    "        timeperiod2=14,\n",
    "        timeperiod3=28,\n",
    "    )\n",
    "\n",
    "    # Add Accumulation/Distribution Line (ADL)\n",
    "    df[\"tlb_ADL\"] = talib.AD(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"])\n",
    "\n",
    "    # Add Average Directional Index (ADX)\n",
    "    df[\"tlb_ADX\"] = talib.ADX(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_atr(df, period):\n",
    "    high_low = df[\"high\"] - df[\"low\"]\n",
    "    high_close = (df[\"high\"] - df[\"close\"].shift()).abs()\n",
    "    low_close = (df[\"low\"] - df[\"close\"].shift()).abs()\n",
    "    ranges = pd.concat([high_low, high_close, low_close], axis=1)\n",
    "    true_range = ranges.max(axis=1)\n",
    "    atr = true_range.rolling(window=period).mean()\n",
    "    return atr\n",
    "\n",
    "\n",
    "def atr(df, period):\n",
    "    atr = ta.volatility.AverageTrueRange(\n",
    "        df[\"high\"], df[\"low\"], df[\"close\"], window=period\n",
    "    )\n",
    "    return atr.average_true_range()\n",
    "\n",
    "\n",
    "def pnf(df, box_size, reversal):\n",
    "    df[\"pnf_high\"] = np.nan\n",
    "    df[\"pnf_low\"] = np.nan\n",
    "    df[\"pnf_direction\"] = 0\n",
    "    df[\"pnf_dtb\"] = 0\n",
    "    df[\"pnf_dbb\"] = 0\n",
    "    df[\"pnf_num_reversals\"] = 0\n",
    "    df[\"pnf_avg_column_height\"] = np.nan\n",
    "    df[\"pnf_max_column_height\"] = np.nan\n",
    "    df[\"pnf_min_column_height\"] = np.nan\n",
    "    df[\"pnf_std_column_height\"] = np.nan\n",
    "    df[\"pnf_bullish_percentage\"] = np.nan\n",
    "    df[\"pnf_bearish_percentage\"] = np.nan\n",
    "    df[\"pnf_num_bullish_to_bearish_reversals\"] = 0\n",
    "    df[\"pnf_num_bearish_to_bullish_reversals\"] = 0\n",
    "    df[\"pnf_avg_bullish_run_length\"] = np.nan\n",
    "\n",
    "    column_heights = []\n",
    "    bullish_run_lengths = []\n",
    "    num_bullish_periods = 0\n",
    "    num_bearish_periods = 0\n",
    "    current_bullish_run_length = 0\n",
    "\n",
    "    current_direction = 0\n",
    "    current_high = df[\"high\"].iloc[0]\n",
    "    current_low = df[\"low\"].iloc[0]\n",
    "    last_high = current_high\n",
    "    last_low = current_low\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        if df[\"high\"].iloc[i] >= current_high + box_size:\n",
    "            current_direction = 1\n",
    "            last_high = current_high\n",
    "            current_high = df[\"high\"].iloc[i]\n",
    "            df.loc[df.index[i], \"pnf_high\"] = current_high\n",
    "            df.loc[df.index[i], \"pnf_direction\"] = current_direction\n",
    "            if last_high is not None and current_high > last_high:\n",
    "                df.loc[df.index[i], \"pnf_dtb\"] = 1  # double top breakout\n",
    "            column_heights.append(current_high - last_high)\n",
    "            current_bullish_run_length += 1\n",
    "            num_bullish_periods += 1\n",
    "        elif df[\"low\"].iloc[i] <= current_low - box_size:\n",
    "            current_direction = -1\n",
    "            last_low = current_low\n",
    "            current_low = df[\"low\"].iloc[i]\n",
    "            df.loc[df.index[i], \"pnf_low\"] = current_low\n",
    "            df.loc[df.index[i], \"pnf_direction\"] = current_direction\n",
    "            if last_low is not None and current_low < last_low:\n",
    "                df.loc[df.index[i], \"pnf_dbb\"] = 1  # double bottom breakdown\n",
    "            column_heights.append(last_low - current_low)\n",
    "            bullish_run_lengths.append(current_bullish_run_length)\n",
    "            current_bullish_run_length = 0\n",
    "            num_bearish_periods += 1\n",
    "        elif (\n",
    "            current_direction == 1\n",
    "            and df[\"low\"].iloc[i] <= current_high - box_size * reversal\n",
    "        ):\n",
    "            current_direction = -1\n",
    "            last_low = current_low\n",
    "            current_low = df[\"low\"].iloc[i]\n",
    "            df.loc[df.index[i], \"pnf_low\"] = current_low\n",
    "            df.loc[df.index[i], \"pnf_direction\"] = current_direction\n",
    "            df.loc[df.index[i], \"pnf_num_bullish_to_bearish_reversals\"] += 1\n",
    "            column_heights.append(last_low - current_low)\n",
    "            bullish_run_lengths.append(current_bullish_run_length)\n",
    "            current_bullish_run_length = 0\n",
    "            num_bearish_periods += 1\n",
    "        elif (\n",
    "            current_direction == -1\n",
    "            and df[\"high\"].iloc[i] >= current_low + box_size * reversal\n",
    "        ):\n",
    "            current_direction = 1\n",
    "            last_high = current_high\n",
    "            current_high = df[\"high\"].iloc[i]\n",
    "            df.loc[df.index[i], \"pnf_high\"] = current_high\n",
    "            df.loc[df.index[i], \"pnf_direction\"] = current_direction\n",
    "            df.loc[df.index[i], \"pnf_num_bearish_to_bullish_reversals\"] += 1\n",
    "            column_heights.append(current_high - last_high)\n",
    "            current_bullish_run_length += 1\n",
    "            num_bullish_periods += 1\n",
    "\n",
    "        df.loc[df.index[i], \"pnf_num_reversals\"] = (\n",
    "            df.loc[df.index[i], \"pnf_num_bearish_to_bullish_reversals\"]\n",
    "            + df.loc[df.index[i], \"pnf_num_bullish_to_bearish_reversals\"]\n",
    "        )\n",
    "        df.loc[df.index[i], \"pnf_avg_column_height\"] = (\n",
    "            np.nan if not column_heights else np.mean(column_heights)\n",
    "        )\n",
    "        df.loc[df.index[i], \"pnf_max_column_height\"] = (\n",
    "            np.nan if not column_heights else np.max(column_heights)\n",
    "        )\n",
    "        df.loc[df.index[i], \"pnf_min_column_height\"] = (\n",
    "            np.nan if not column_heights else np.min(column_heights)\n",
    "        )\n",
    "        df.loc[df.index[i], \"pnf_std_column_height\"] = (\n",
    "            np.nan if not column_heights else np.std(column_heights)\n",
    "        )\n",
    "        df.loc[df.index[i], \"pnf_bullish_percentage\"] = (\n",
    "            np.nan\n",
    "            if not num_bullish_periods\n",
    "            else num_bullish_periods / (num_bullish_periods + num_bearish_periods)\n",
    "        )\n",
    "        df.loc[df.index[i], \"pnf_bearish_percentage\"] = (\n",
    "            np.nan\n",
    "            if not num_bearish_periods\n",
    "            else num_bearish_periods / (num_bullish_periods + num_bearish_periods)\n",
    "        )\n",
    "        df.loc[df.index[i], \"pnf_avg_bullish_run_length\"] = (\n",
    "            np.nan if not bullish_run_lengths else np.mean(bullish_run_lengths)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    ## DIFFERENCES ##\n",
    "\n",
    "    # Price differences\n",
    "    df[\"price_diff\"] = df[\"close\"].diff()\n",
    "    df[\"op_cl_diff\"] = df[\"open\"] - df[\"close\"]\n",
    "\n",
    "    # Moving averages\n",
    "    df[\"ma_5\"] = df[\"close\"].rolling(window=5).mean()\n",
    "    df[\"ma_10\"] = df[\"close\"].rolling(window=10).mean()\n",
    "\n",
    "    # Price percentage change\n",
    "    df[\"pct_change\"] = df[\"close\"].pct_change()\n",
    "\n",
    "    # RSI\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df[\"rsi\"] = 100 - 100 / (1 + rs)\n",
    "\n",
    "    # Other popular difference features\n",
    "    for i in range(1, 35):\n",
    "        df[\"diff_{}\".format(i)] = df[\"close\"].diff(i)\n",
    "\n",
    "    sma = df[\"close\"].rolling(window=20).mean()\n",
    "    std = df[\"close\"].rolling(window=20).std()\n",
    "    df[\"upper_band\"] = sma + (2 * std)\n",
    "    df[\"lower_band\"] = sma - (2 * std)\n",
    "\n",
    "    highest_high = df[\"high\"].rolling(window=14).max()\n",
    "    lowest_low = df[\"low\"].rolling(window=14).min()\n",
    "    df[\"williams_r\"] = (highest_high - df[\"close\"]) / (highest_high - lowest_low) * -100\n",
    "\n",
    "    df[\"obv\"] = (np.sign(df[\"close\"].diff()) * df[\"volume\"]).fillna(0).cumsum()\n",
    "\n",
    "    tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "    sma = tp.rolling(window=20).mean()\n",
    "    mean_deviation = tp.rolling(window=20).apply(\n",
    "        lambda x: np.mean(np.abs(x - x.mean()))\n",
    "    )\n",
    "    df[\"cci\"] = (tp - sma) / (0.015 * mean_deviation)\n",
    "\n",
    "    true_range = pd.concat(\n",
    "        [\n",
    "            df[\"high\"] - df[\"low\"],\n",
    "            (df[\"high\"] - df[\"close\"].shift()).abs(),\n",
    "            (df[\"close\"].shift() - df[\"low\"]).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1)\n",
    "    df[\"atr\"] = true_range.rolling(window=14).mean()\n",
    "\n",
    "    money_flow_vol = (\n",
    "        ((df[\"close\"] - df[\"low\"]) - (df[\"high\"] - df[\"close\"]))\n",
    "        / (df[\"high\"] - df[\"low\"])\n",
    "        * df[\"volume\"]\n",
    "    )\n",
    "    cmf = (\n",
    "        money_flow_vol.rolling(window=20).sum() / df[\"volume\"].rolling(window=20).sum()\n",
    "    )\n",
    "    df[\"cmf\"] = cmf\n",
    "\n",
    "    ema_12 = df[\"close\"].ewm(span=12).mean()\n",
    "    ema_26 = df[\"close\"].ewm(span=26).mean()\n",
    "    ppo = (ema_12 - ema_26) / ema_26 * 100\n",
    "    df[\"ppo\"] = ppo\n",
    "\n",
    "    ### LAG ###\n",
    "\n",
    "    # Lag closing prices\n",
    "    for i in range(1, 11):\n",
    "        df[\"lag_close_{}\".format(i)] = df[\"close\"].shift(i)\n",
    "\n",
    "    # Lag daily returns\n",
    "    returns = df[\"close\"].pct_change()\n",
    "    for i in range(1, 6):\n",
    "        df[\"lag_return_{}\".format(i)] = returns.shift(i)\n",
    "\n",
    "    # Lag high and low prices\n",
    "    for i in range(1, 4):\n",
    "        df[\"lag_high_{}\".format(i)] = df[\"high\"].shift(i)\n",
    "        df[\"lag_low_{}\".format(i)] = df[\"low\"].shift(i)\n",
    "\n",
    "    # Historical volatility\n",
    "    df[\"hist_volatility_10\"] = returns.rolling(window=10).std()\n",
    "    df[\"hist_volatility_20\"] = returns.rolling(window=20).std()\n",
    "    df[\"hist_volatility_30\"] = returns.rolling(window=30).std()\n",
    "\n",
    "    # Previous day's RSI value\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - 100 / (1 + rs)\n",
    "    df[\"lag_rsi_1\"] = rsi.shift(1)\n",
    "\n",
    "    ### ROLLING ###\n",
    "\n",
    "    # Moving averages\n",
    "    df[\"ma_5\"] = df[\"close\"].rolling(window=5).mean()\n",
    "    df[\"ma_10\"] = df[\"close\"].rolling(window=10).mean()\n",
    "    df[\"ma_20\"] = df[\"close\"].rolling(window=20).mean()\n",
    "\n",
    "    # Exponential moving averages\n",
    "    df[\"ema_5\"] = df[\"close\"].ewm(span=5).mean()\n",
    "    df[\"ema_10\"] = df[\"close\"].ewm(span=10).mean()\n",
    "    df[\"ema_20\"] = df[\"close\"].ewm(span=20).mean()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    sma = df[\"close\"].rolling(window=20).mean()\n",
    "    std = df[\"close\"].rolling(window=20).std()\n",
    "    df[\"upper_band\"] = sma + (2 * std)\n",
    "    df[\"lower_band\"] = sma - (2 * std)\n",
    "\n",
    "    # Rate of Change (ROC)\n",
    "    df[\"roc_5\"] = df[\"close\"].pct_change(periods=5)\n",
    "    df[\"roc_10\"] = df[\"close\"].pct_change(periods=10)\n",
    "\n",
    "    # Standard deviation\n",
    "    df[\"std_5\"] = df[\"close\"].rolling(window=5).std()\n",
    "    df[\"std_10\"] = df[\"close\"].rolling(window=10).std()\n",
    "\n",
    "    # Average True Range (ATR)\n",
    "    true_range = pd.concat(\n",
    "        [\n",
    "            df[\"high\"] - df[\"low\"],\n",
    "            (df[\"high\"] - df[\"close\"].shift()).abs(),\n",
    "            (df[\"low\"] - df[\"close\"].shift()).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1)\n",
    "    df[\"atr_14\"] = true_range.rolling(window=14).mean()\n",
    "\n",
    "    # Keltner Channels\n",
    "    middle_line = df[\"close\"].rolling(window=20).mean()\n",
    "    upper_keltner = middle_line + 2 * df[\"atr_14\"]\n",
    "    lower_keltner = middle_line - 2 * df[\"atr_14\"]\n",
    "    df[\"upper_keltner\"] = upper_keltner\n",
    "    df[\"lower_keltner\"] = lower_keltner\n",
    "\n",
    "    # RSI\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df[\"rsi\"] = 100 - 100 / (1 + rs)\n",
    "\n",
    "    ### RATIO ###\n",
    "\n",
    "    # Calculate some basic features first\n",
    "    returns = df[\"close\"].pct_change()\n",
    "    ema_12 = df[\"close\"].ewm(span=12).mean()\n",
    "    ema_26 = df[\"close\"].ewm(span=26).mean()\n",
    "\n",
    "    # 1. Price-to-moving-average ratios\n",
    "    df[\"price_to_ma_5\"] = df[\"close\"] / df[\"ma_5\"]\n",
    "    df[\"price_to_ma_10\"] = df[\"close\"] / df[\"ma_10\"]\n",
    "    df[\"price_to_ma_20\"] = df[\"close\"] / df[\"ma_20\"]\n",
    "\n",
    "    # 2. Price-to-EMA ratios\n",
    "    df[\"price_to_ema_5\"] = df[\"close\"] / df[\"ema_5\"]\n",
    "    df[\"price_to_ema_10\"] = df[\"close\"] / df[\"ema_10\"]\n",
    "    df[\"price_to_ema_20\"] = df[\"close\"] / df[\"ema_20\"]\n",
    "\n",
    "    # 3. EMA-to-moving-average ratios\n",
    "    df[\"ema_to_ma_5\"] = df[\"ema_5\"] / df[\"ma_5\"]\n",
    "    df[\"ema_to_ma_10\"] = df[\"ema_10\"] / df[\"ma_10\"]\n",
    "    df[\"ema_to_ma_20\"] = df[\"ema_20\"] / df[\"ma_20\"]\n",
    "\n",
    "    # 4. EMA-to-EMA ratios\n",
    "    df[\"ema5_to_ema10\"] = df[\"ema_5\"] / df[\"ema_10\"]\n",
    "    df[\"ema5_to_ema20\"] = df[\"ema_5\"] / df[\"ema_20\"]\n",
    "    df[\"ema10_to_ema20\"] = df[\"ema_10\"] / df[\"ema_20\"]\n",
    "\n",
    "    # 5. Moving-average-to-moving-average ratios\n",
    "    df[\"ma5_to_ma10\"] = df[\"ma_5\"] / df[\"ma_10\"]\n",
    "    df[\"ma5_to_ma20\"] = df[\"ma_5\"] / df[\"ma_20\"]\n",
    "    df[\"ma10_to_ma20\"] = df[\"ma_10\"] / df[\"ma_20\"]\n",
    "\n",
    "    # 6. Price-to-Bollinger-Band ratios\n",
    "    df[\"price_to_upper_band\"] = df[\"close\"] / df[\"upper_band\"]\n",
    "    df[\"price_to_lower_band\"] = df[\"close\"] / df[\"lower_band\"]\n",
    "\n",
    "    # 7. Price-to-Keltner-Channel ratios\n",
    "    df[\"price_to_upper_keltner\"] = df[\"close\"] / df[\"upper_keltner\"]\n",
    "    df[\"price_to_lower_keltner\"] = df[\"close\"] / df[\"lower_keltner\"]\n",
    "\n",
    "    # 8. Price-to-previous-day-high ratio\n",
    "    df[\"price_to_prev_high\"] = df[\"close\"] / df[\"high\"].shift(1)\n",
    "\n",
    "    # 9. Price-to-previous-day-low ratio\n",
    "    df[\"price_to_prev_low\"] = df[\"close\"] / df[\"low\"].shift(1)\n",
    "\n",
    "    # 10. RSI-to-moving-average ratio\n",
    "    df[\"rsi_to_ma_5\"] = df[\"rsi\"] / df[\"ma_5\"]\n",
    "\n",
    "    ### CORRELATION ####\n",
    "\n",
    "    # Calculate some basic features first\n",
    "    returns = df[\"close\"].pct_change()\n",
    "    volume_change = df[\"volume\"].pct_change()\n",
    "\n",
    "    # 1. Correlation with returns and volume (short-term)\n",
    "    df[\"return_volume_corr_5\"] = returns.rolling(window=5).corr(df[\"volume\"])\n",
    "    df[\"return_volume_corr_10\"] = returns.rolling(window=10).corr(df[\"volume\"])\n",
    "    df[\"return_volume_corr_20\"] = returns.rolling(window=20).corr(df[\"volume\"])\n",
    "\n",
    "    # 2. Correlation with returns and volume (medium-term)\n",
    "    df[\"return_volume_corr_50\"] = returns.rolling(window=50).corr(df[\"volume\"])\n",
    "    df[\"return_volume_corr_100\"] = returns.rolling(window=100).corr(df[\"volume\"])\n",
    "\n",
    "    # 3. Correlation with price and volume\n",
    "    df[\"price_volume_corr_5\"] = df[\"close\"].rolling(window=5).corr(df[\"volume\"])\n",
    "    df[\"price_volume_corr_10\"] = df[\"close\"].rolling(window=10).corr(df[\"volume\"])\n",
    "    df[\"price_volume_corr_20\"] = df[\"close\"].rolling(window=20).corr(df[\"volume\"])\n",
    "\n",
    "    # 4. Correlation with price and volume (medium-term)\n",
    "    df[\"price_volume_corr_50\"] = df[\"close\"].rolling(window=50).corr(df[\"volume\"])\n",
    "    df[\"price_volume_corr_100\"] = df[\"close\"].rolling(window=100).corr(df[\"volume\"])\n",
    "\n",
    "    # 5. Price / volume\n",
    "    df[\"price_to_volume\"] = df[\"close\"] / df[\"volume\"]\n",
    "\n",
    "    # 6. Volume Relative Strength Index (VRSI)\n",
    "    delta_vol = df[\"volume\"].diff()\n",
    "    gain_vol = delta_vol.where(delta_vol > 0, 0)\n",
    "    loss_vol = -delta_vol.where(delta_vol < 0, 0)\n",
    "    avg_gain_vol = gain_vol.rolling(window=14).mean()\n",
    "    avg_loss_vol = loss_vol.rolling(window=14).mean()\n",
    "    rs_vol = avg_gain_vol / avg_loss_vol\n",
    "    df[\"vrsi\"] = 100 - 100 / (1 + rs_vol)\n",
    "\n",
    "    # 7. Change in volume\n",
    "    df[\"volume_change\"] = volume_change\n",
    "\n",
    "    # 8. Moving averages of volume\n",
    "    df[\"volume_ma_5\"] = df[\"volume\"].rolling(window=5).mean()\n",
    "    df[\"volume_ma_10\"] = df[\"volume\"].rolling(window=10).mean()\n",
    "    df[\"volume_ma_20\"] = df[\"volume\"].rolling(window=20).mean()\n",
    "\n",
    "    # 9. Standard deviation of volume\n",
    "    df[\"volume_std_5\"] = df[\"volume\"].rolling(window=5).std()\n",
    "    df[\"volume_std_10\"] = df[\"volume\"].rolling(window=10).std()\n",
    "    df[\"volume_std_20\"] = df[\"volume\"].rolling(window=20).std()\n",
    "\n",
    "    # 10. Ratio of volume to moving average volume\n",
    "    df[\"volume_to_ma_volume\"] = df[\"volume\"] / df[\"volume_ma_20\"]\n",
    "\n",
    "    ### EXTRA ###\n",
    "\n",
    "    # Calculate necessary base features\n",
    "    returns = df[\"close\"].pct_change()\n",
    "    delta = df[\"close\"].diff()\n",
    "\n",
    "    # close_change\n",
    "    df[\"close_change\"] = df[\"close\"].diff()\n",
    "\n",
    "    # high_pct\n",
    "    df[\"high_pct\"] = df[\"high\"].pct_change()\n",
    "\n",
    "    # close_change_roll5\n",
    "    df[\"close_change_roll5\"] = df[\"close_change\"].rolling(window=5).mean()\n",
    "\n",
    "    # RSI_14_roll5\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - 100 / (1 + rs)\n",
    "    df[\"RSI_14_roll5\"] = rsi.rolling(window=5).mean()\n",
    "\n",
    "    # ATR_14_roll5\n",
    "    true_range = pd.concat(\n",
    "        [\n",
    "            df[\"high\"] - df[\"low\"],\n",
    "            (df[\"high\"] - df[\"close\"].shift()).abs(),\n",
    "            (df[\"low\"] - df[\"close\"].shift()).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1)\n",
    "    df[\"ATR_14_roll5\"] = true_range.rolling(window=14).mean().rolling(window=5).mean()\n",
    "\n",
    "    # volume_roll5\n",
    "    df[\"volume_roll5\"] = df[\"volume\"].rolling(window=5).mean()\n",
    "\n",
    "    # high_pct_roll5\n",
    "    df[\"high_pct_roll5\"] = df[\"high_pct\"].rolling(window=5).mean()\n",
    "\n",
    "    # volatility_5\n",
    "    df[\"volatility_5\"] = df[\"close\"].rolling(window=5).std()\n",
    "\n",
    "    # price_ema5\n",
    "    df[\"price_ema5\"] = df[\"close\"].ewm(span=5).mean()\n",
    "\n",
    "    # volume_ema5\n",
    "    df[\"volume_ema5\"] = df[\"volume\"].ewm(span=5).mean()\n",
    "\n",
    "    # price_to_ema5\n",
    "    df[\"price_to_ema5\"] = df[\"close\"] / df[\"price_ema5\"]\n",
    "\n",
    "    # volume_change_roll5\n",
    "    df[\"volume_change_roll5\"] = volume_change.rolling(window=5).mean()\n",
    "\n",
    "    # avg_vol_last_100\n",
    "    df[\"avg_vol_last_100\"] = df[\"volume\"].rolling(window=100).mean()\n",
    "\n",
    "    # turnover\n",
    "    df[\"turnover\"] = df[\"volume\"] * df[\"close\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def timeseries_cv_score(X, y, n_splits):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    f1_scores = []\n",
    "    auc_scores = []  # list to store ROC AUC scores for each split\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))  # because of binary classification\n",
    "\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "        # Calculate F1 score of the model on the test set\n",
    "        f1 = f1_score(y_test, (y_pred > 0.5).astype(\"int32\"))\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Calculate ROC AUC score of the model on the test set\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    return np.mean(f1_scores), np.mean(auc_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to show all rows and columns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Prepare TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "# Load the data\n",
    "data_path = \"../../../data/kc/btc/raw/kc_btc_15min.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ATR'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ATR'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[\u001b[39m~\u001b[39mdf\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mduplicated(keep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m     13\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mpnf_ATR\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m custom_atr(df, \u001b[39m14\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mpnf_ATR\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mATR\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39mfillna(method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mffill\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mfillna(method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbfill\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m atr_value \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mATR\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     16\u001b[0m box_size \u001b[39m=\u001b[39m atr_value \u001b[39m/\u001b[39m \u001b[39m6\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\jnorm\\Projects\\websocket_trading\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ATR'"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "df = load_data(data_path)\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# print(df.columns)\n",
    "\n",
    "# Timestamp conversion and index setting\n",
    "df[\"time_unix\"] = df[\"time\"]\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\")  # Assuming 'time' is in seconds\n",
    "df.set_index(\"time\", inplace=True)\n",
    "df = df.loc[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "df[\"pnf_ATR\"] = custom_atr(df, 14)\n",
    "df[\"pnf_ATR\"] = df[\"pnf_ATR\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "atr_value = df[\"pnf_ATR\"].iloc[-1]\n",
    "box_size = atr_value / 6\n",
    "\n",
    "# Use the PNF function\n",
    "pnf_df = pnf(df, box_size, atr_value / 2)\n",
    "direction_changes = pnf_df[\"pnf_direction\"].ne(0).sum()\n",
    "print(f\"Number of direction changes: {direction_changes}\")\n",
    "\n",
    "# Check the result\n",
    "# print(df)\n",
    "print(df.tail(100))\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "# pnf_df.tail(10)\n",
    "\n",
    "\n",
    "# # Technical Analysis features\n",
    "# df.ta.strategy(\"all\")\n",
    "\n",
    "# # Check your results and exclude as necessary.\n",
    "# df.ta.strategy(fast=10, slow=50, verbose=True)\n",
    "\n",
    "\n",
    "# # Heiken Ashi\n",
    "# df = add_heiken_ashi_features(df)\n",
    "\n",
    "# print(\"'time_unix' in df after Heiken Ashi features: \", \"time_unix\" in df.columns)\n",
    "# print(df[\"time_unix\"].head())\n",
    "# print(\"Rows in df after Heiken Ashi features: \", len(df))\n",
    "\n",
    "\n",
    "# ## RENKO\n",
    "# # CONSIDER CHANGING multiplyer to .5 or even 1\n",
    "# renko_df = compute_renko(df)\n",
    "\n",
    "# print(\"'time_unix' in renko_df: \", \"time_unix\" in renko_df.columns)\n",
    "# print(renko_df[\"time_unix\"].head())\n",
    "# print(\"Rows in renko_df: \", len(renko_df))\n",
    "\n",
    "# df = add_renko_features(df, renko_df)\n",
    "\n",
    "# print(\"'time_unix' in df after Renko features: \", \"time_unix\" in df.columns)\n",
    "# print(df[\"time_unix\"].head())\n",
    "# print(\"Rows in df after Renko features: \", len(df))\n",
    "\n",
    "# ## KAGI ##\n",
    "\n",
    "# kagi_df = kagi(df)\n",
    "# print(\"'time_unix' in kagi_df: \", \"time_unix\" in kagi_df.columns)\n",
    "# print(kagi_df[\"time_unix\"].head())\n",
    "# print(\"Rows in kagi_df: \", len(kagi_df))\n",
    "\n",
    "# # Add Kagi features\n",
    "# kagi_df = add_kagi_features(df, kagi_df)\n",
    "\n",
    "# print(\"'time_unix' in kagi_df after Kagi features: \", \"time_unix\" in kagi_df.columns)\n",
    "# print(kagi_df[\"time_unix\"].head())\n",
    "# print(\"Rows in kagi_df after Kagi features: \", len(kagi_df))\n",
    "\n",
    "# df = df.join(kagi_df.set_index(\"time_unix\"), on=\"time_unix\")\n",
    "\n",
    "# print(\"'time_unix' in df after joining with kagi_df: \", \"time_unix\" in df.columns)\n",
    "# print(df[\"time_unix\"].head())\n",
    "# print(\"Rows in df after joining with kagi_df: \", len(df))\n",
    "\n",
    "# ## Three Line Break ##\n",
    "\n",
    "# df_tlb = three_line_break(df)\n",
    "# direction_reversals = (\n",
    "#     df_tlb[\"tlb_direction\"].shift(1) != df_tlb[\"tlb_direction\"]\n",
    "# ).sum()\n",
    "# print(\"The number of direction reversals:\", direction_reversals)\n",
    "\n",
    "\n",
    "# # Create the TLB dataframe\n",
    "# tlb_df = three_line_break(df)\n",
    "# print(\"'time_unix' in tlb_df: \", \"time_unix\" in tlb_df.columns)\n",
    "# print(tlb_df[\"time_unix\"].head())\n",
    "# print(\"Rows in tlb_df: \", len(tlb_df))\n",
    "\n",
    "# # Add TLB features\n",
    "# tlb_df = add_tlb_features(tlb_df)\n",
    "\n",
    "# print(\"'time_unix' in tlb_df after TLB features: \", \"time_unix\" in tlb_df.columns)\n",
    "# print(tlb_df[\"time_unix\"].head())\n",
    "# print(\"Rows in tlb_df after TLB features: \", len(tlb_df))\n",
    "\n",
    "# # Join TLB dataframe with original dataframe\n",
    "# df = df.join(tlb_df.set_index(\"time_unix\"), on=\"time_unix\", rsuffix=\"_tlb\")\n",
    "\n",
    "# print(\"'time_unix' in df after joining with tlb_df: \", \"time_unix\" in df.columns)\n",
    "# print(df[\"time_unix\"].head())\n",
    "# print(\"Rows in df after joining with tlb_df: \", len(df))\n",
    "\n",
    "\n",
    "# print(df.columns)\n",
    "\n",
    "\n",
    "# # Additional features\n",
    "# df = create_features(df)\n",
    "\n",
    "# # Now extract additional date information from the original dataframe\n",
    "# df[\"minute\"] = df.index.minute\n",
    "# df[\"hour\"] = df.index.hour\n",
    "# df[\"day\"] = df.index.day\n",
    "# df[\"month\"] = df.index.month\n",
    "\n",
    "# # Sanity check. Make sure all the columns and types\n",
    "# # print(df.columns)\n",
    "\n",
    "# # Forward Fill\n",
    "# df.ffill(inplace=True)\n",
    "\n",
    "# # Backward Fill\n",
    "# df.bfill(inplace=True)\n",
    "\n",
    "# # # Finding problem features for standard scalar\n",
    "# # non_num_features = df.select_dtypes(exclude=[\"int32\", \"int64\", \"float32\", \"float64\"])\n",
    "# # print(\"Fix these features:\\n\")\n",
    "# # for col, dtype in non_num_features.dtypes.items():\n",
    "# #     print(f\"{col}: {dtype}\")\n",
    "\n",
    "\n",
    "# # Select numeric columns which need to be scaled\n",
    "# do_not_scale_columns = [\n",
    "#     \"time_unix\",\n",
    "#     \"minute\",\n",
    "#     \"hour\",\n",
    "#     \"day\",\n",
    "#     \"month\",\n",
    "# ]\n",
    "# scaler = StandardScaler()\n",
    "# for col in df.columns:\n",
    "#     if col not in do_not_scale_columns:\n",
    "#         df[col] = scaler.fit_transform(df[[col]])\n",
    "\n",
    "\n",
    "# X = df.drop(\"color_change\", axis=1)\n",
    "# y = df[\"color_change\"]\n",
    "\n",
    "# df.tail()\n",
    "# duplicate features\n",
    "# duplicated_features = df.columns.duplicated()\n",
    "# print(\"Duplicate Features: \", df.columns[duplicated_features])\n",
    "# Total features\n",
    "# print(\"Total features in DataFrame: \", df.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Re-scale the data to include the new feature\u001b[39;00m\n\u001b[0;32m      2\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 3\u001b[0m X_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X)\n\u001b[0;32m      5\u001b[0m \u001b[39m# feature selection\u001b[39;00m\n\u001b[0;32m      6\u001b[0m selector \u001b[39m=\u001b[39m SelectKBest(score_func\u001b[39m=\u001b[39mf_classif, k\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Re-scale the data to include the new feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "selector.fit(X_scaled, y)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "features_df_new = X.iloc[:, cols]\n",
    "\n",
    "# Store the scores of each feature in a dictionary\n",
    "feature_scores = {\n",
    "    feature_name: score for feature_name, score in zip(X.columns, selector.scores_)\n",
    "}\n",
    "\n",
    "# Sort the dictionary by value in descending order and print the scores\n",
    "for feature_name, score in sorted(\n",
    "    feature_scores.items(), key=lambda item: item[1], reverse=True\n",
    "):\n",
    "    print(f\"{feature_name}: {score}\")\n",
    "\n",
    "# Now we can apply Logistic Regression and Random Forests on the new features_df_new\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, features_df_new, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "print(f\"\\nLogistic Regression CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, features_df_new, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "print(f\"Random Forest CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "X_array = X.values\n",
    "X_reshaped = X_array.reshape((X_array.shape[0], 1, X_array.shape[1]))\n",
    "\n",
    "# Call the function\n",
    "mean_f1_score = timeseries_cv_score(X_reshaped, y.values, n_splits=5)\n",
    "print(f\"\\nLSTM CV F1 score: {mean_f1_score}\")\n",
    "\n",
    "print(\"\\n\", features_df_new.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m log_reg \u001b[39m=\u001b[39m LogisticRegression(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Cross-validation\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m cv_scores \u001b[39m=\u001b[39m cross_val_score(log_reg, X, y, cv\u001b[39m=\u001b[39mtscv, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mroc_auc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLogistic Regression CV ROC AUC score: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(cv_scores)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Random Forest\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Logistic Regression CV ROC AUC score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Random Forest CV ROC AUC score: {np.mean(cv_scores)}\")\n",
    "\n",
    "print(\"\\n\", X.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 812us/step\n",
      "42/42 [==============================] - 0s 842us/step\n",
      "42/42 [==============================] - 0s 781us/step\n",
      "42/42 [==============================] - 0s 842us/step\n",
      "42/42 [==============================] - 0s 720us/step\n",
      "\n",
      "LSTM CV ROC AUC score: (0.6953945419128389, 0.4942367515727568)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "X_array = X.values\n",
    "X_reshaped = X_array.reshape((X_array.shape[0], 1, X_array.shape[1]))\n",
    "\n",
    "# Call the function\n",
    "mean_auc_score = timeseries_cv_score(X_reshaped, y.values, n_splits=5)\n",
    "print(f\"\\nLSTM CV ROC AUC score: {mean_auc_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

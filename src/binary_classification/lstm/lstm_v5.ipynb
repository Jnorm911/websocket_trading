{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import talib\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df[\"color_change\"] = df[\"color\"].diff().ne(0).astype(int)\n",
    "    df[\"color_change\"].fillna(0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def scale_data(df):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "def add_heiken_ashi_features(df):\n",
    "    # Create Heiken Ashi DataFrame\n",
    "    ha_df = df.ta.ha()\n",
    "\n",
    "    # Rename the HA columns\n",
    "    ha_df.columns = [f\"HA_{col}\" for col in ha_df.columns]\n",
    "\n",
    "    # Join the HA columns to the original dataframe\n",
    "    df = df.join(ha_df)\n",
    "\n",
    "    # Heiken Ashi Close to Open\n",
    "    df[\"HA_close_open\"] = df[\"HA_close\"] - df[\"HA_open\"]\n",
    "\n",
    "    # Heiken Ashi High Low Range\n",
    "    df[\"HA_high_low\"] = df[\"HA_high\"] - df[\"HA_low\"]\n",
    "\n",
    "    # Heiken Ashi Body Range\n",
    "    df[\"HA_body\"] = abs(df[\"HA_close\"] - df[\"HA_open\"])\n",
    "\n",
    "    # Heiken Ashi Price Direction\n",
    "    df[\"HA_direction\"] = (df[\"HA_close\"] > df[\"HA_open\"]).astype(int)\n",
    "\n",
    "    # Heiken Ashi Volume-weighted Price\n",
    "    df[\"HA_vwap\"] = (df[\"HA_close\"] * df[\"volume\"]).cumsum() / df[\"volume\"].cumsum()\n",
    "\n",
    "    # Lag 1 feature\n",
    "    df[\"HA_close_lag1\"] = df[\"HA_close\"].shift(1)\n",
    "\n",
    "    # Close Change\n",
    "    df[\"HA_close_change\"] = df[\"HA_close\"].diff()\n",
    "\n",
    "    # Close % Change\n",
    "    df[\"HA_close_pct_change\"] = df[\"HA_close\"].pct_change()\n",
    "\n",
    "    # 5-period Simple Moving Average\n",
    "    df[\"HA_sma5\"] = df[\"HA_close\"].rolling(5).mean()\n",
    "\n",
    "    # 5-period Exponential Moving Average\n",
    "    df[\"HA_ema5\"] = df[\"HA_close\"].ewm(span=5).mean()\n",
    "\n",
    "    # Additional features\n",
    "    df[\"HA_ema10\"] = df[\"HA_close\"].ewm(span=10).mean()\n",
    "    df[\"HA_ema15\"] = df[\"HA_close\"].ewm(span=15).mean()\n",
    "    df[\"HA_pct_diff_ema5_15\"] = (\n",
    "        (df[\"HA_ema5\"] - df[\"HA_ema15\"]) / df[\"HA_ema15\"]\n",
    "    ) * 100\n",
    "    df[\"HA_rsi\"] = ta.rsi(df[\"HA_close\"])\n",
    "    # Calculate Short Term Exponential Moving Average\n",
    "    df[\"short_ema\"] = df[\"HA_close\"].ewm(span=12, adjust=False).mean()\n",
    "\n",
    "    # Calculate Long Term Exponential Moving Average\n",
    "    df[\"long_ema\"] = df[\"HA_close\"].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    # Calculate Moving Average Convergence Divergence (MACD)\n",
    "    df[\"HA_macd\"] = df[\"short_ema\"] - df[\"long_ema\"]\n",
    "\n",
    "    # Calculate Signal Line\n",
    "    df[\"HA_macds\"] = df[\"HA_macd\"].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # Calculate MACD Histogram\n",
    "    df[\"HA_macdh\"] = df[\"HA_macd\"] - df[\"HA_macds\"]\n",
    "\n",
    "    # Drop temporary short_ema and long_ema columns\n",
    "    df = df.drop([\"short_ema\", \"long_ema\"], axis=1)\n",
    "\n",
    "    df[\"HA_cci\"] = ta.cci(df[\"HA_high\"], df[\"HA_low\"], df[\"HA_close\"])\n",
    "    df[\"HA_atr\"] = ta.atr(df[\"HA_high\"], df[\"HA_low\"], df[\"HA_close\"])\n",
    "    df[\"HA_ha_close_bbp50_std\"] = (\n",
    "        ta.stdev(df[\"HA_close\"], 50) / df[\"HA_close\"]\n",
    "    )  # Bollinger Bands normalized width\n",
    "    df[\"HA_mfi\"] = ta.mfi(df[\"HA_high\"], df[\"HA_low\"], df[\"HA_close\"], df[\"volume\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def compute_renko(df, timeperiod=14, multiplier=0.25):\n",
    "    # Calculate ATR\n",
    "    atr_values = talib.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod)\n",
    "\n",
    "    # Drop rows with NaN ATR values\n",
    "    df = df[atr_values.notna()]\n",
    "    atr_values = atr_values.dropna()\n",
    "\n",
    "    # Compute average ATR\n",
    "    average_atr = atr_values.mean()\n",
    "\n",
    "    # Set brick size\n",
    "    brick_size = average_atr * multiplier\n",
    "\n",
    "    renko_df = pd.DataFrame(\n",
    "        index=df.index, columns=[\"open\", \"high\", \"low\", \"close\", \"time_unix\"]\n",
    "    )\n",
    "\n",
    "    current_price = df[\"close\"][0]\n",
    "    last_reset_price = current_price\n",
    "\n",
    "    for idx in df.index[1:]:\n",
    "        current_price = df.loc[idx, \"close\"]\n",
    "        renko_df.loc[idx, \"time_unix\"] = df.loc[\n",
    "            idx, \"time_unix\"\n",
    "        ]  # Copy 'time_unix' from df\n",
    "\n",
    "        while last_reset_price + brick_size <= current_price:\n",
    "            renko_df.loc[idx] = [\n",
    "                last_reset_price,\n",
    "                last_reset_price + brick_size,\n",
    "                last_reset_price,\n",
    "                last_reset_price + brick_size,\n",
    "                df.loc[idx, \"time_unix\"],\n",
    "            ]\n",
    "            last_reset_price += brick_size\n",
    "\n",
    "        while last_reset_price - brick_size >= current_price:\n",
    "            renko_df.loc[idx] = [\n",
    "                last_reset_price,\n",
    "                last_reset_price - brick_size,\n",
    "                last_reset_price - brick_size,\n",
    "                last_reset_price,\n",
    "                df.loc[idx, \"time_unix\"],\n",
    "            ]\n",
    "            last_reset_price -= brick_size\n",
    "\n",
    "    return renko_df.dropna()\n",
    "\n",
    "\n",
    "def add_renko_features(df, renko_df):\n",
    "    # Append \"_renko\" to the column names of renko_df\n",
    "    renko_df.columns = [str(col) + \"_renko\" for col in renko_df.columns]\n",
    "\n",
    "    # Add EMA, SMA, WMA, RSI, and Bollinger Bands to renko_df\n",
    "    renko_df[\"sma_renko\"] = talib.SMA(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    renko_df[\"ema_renko\"] = talib.EMA(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    renko_df[\"wma_renko\"] = talib.WMA(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    renko_df[\"rsi_renko\"] = talib.RSI(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    (\n",
    "        renko_df[\"upper_band_renko\"],\n",
    "        renko_df[\"middle_band_renko\"],\n",
    "        renko_df[\"lower_band_renko\"],\n",
    "    ) = talib.BBANDS(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    # Add ROC and Momentum to renko_df\n",
    "    renko_df[\"roc_renko\"] = talib.ROC(renko_df[\"close_renko\"], timeperiod=5)\n",
    "    renko_df[\"momentum_renko\"] = renko_df[\"close_renko\"].diff()\n",
    "\n",
    "    # Add Stochastic Oscillator to renko_df\n",
    "    renko_df[\"k_renko\"], renko_df[\"d_renko\"] = talib.STOCH(\n",
    "        renko_df[\"high_renko\"],\n",
    "        renko_df[\"low_renko\"],\n",
    "        renko_df[\"close_renko\"],\n",
    "        fastk_period=5,\n",
    "        slowk_period=3,\n",
    "        slowd_period=3,\n",
    "    )\n",
    "    df = df.join(\n",
    "        renko_df.set_index(\"time_unix_renko\"), on=\"time_unix\"\n",
    "    )  # Join Renko data with the original DataFrame\n",
    "\n",
    "    # Derived features\n",
    "    df[\"close_open_renko\"] = df[\"close_renko\"] - df[\"open_renko\"]\n",
    "    df[\"high_low_renko\"] = df[\"high_renko\"] - df[\"low_renko\"]\n",
    "    df[\"close_change_renko\"] = df[\"close_renko\"].diff()\n",
    "    df[\"direction_renko\"] = df[\"close_change_renko\"].apply(lambda x: int(x > 0))\n",
    "    df[\"direction_change\"] = df[\"direction_renko\"].diff().abs()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def kagi(df):\n",
    "    kagi_data = []\n",
    "    cur_col = {\n",
    "        \"kagi_dir\": \"up\",\n",
    "        \"kagi_start\": df.loc[df.index[0], \"close\"],\n",
    "        \"time_unix\": df.loc[df.index[0], \"time_unix\"],\n",
    "    }\n",
    "    for index, row in df.iterrows():\n",
    "        cp = row[\"close\"]\n",
    "        diff = cp - cur_col[\"kagi_start\"]\n",
    "        if cur_col[\"kagi_dir\"] == \"up\":\n",
    "            if diff < 0:\n",
    "                kagi_data.append(cur_col.copy())\n",
    "                cur_col[\"kagi_dir\"] = \"down\"\n",
    "        else:\n",
    "            if diff > 0:\n",
    "                kagi_data.append(cur_col.copy())\n",
    "                cur_col[\"kagi_dir\"] = \"up\"\n",
    "        cur_col[\"kagi_start\"] = cp\n",
    "        cur_col[\"time_unix\"] = row[\"time_unix\"]\n",
    "    kagi_data.append(cur_col)\n",
    "    kagi_df = pd.DataFrame(kagi_data)\n",
    "    return kagi_df\n",
    "\n",
    "\n",
    "def add_kagi_features(df, kagi_df):\n",
    "    # Calculate simple moving averages\n",
    "    kagi_df[\"kagi_sma5\"] = talib.SMA(kagi_df[\"kagi_start\"], timeperiod=5)\n",
    "    kagi_df[\"kagi_sma10\"] = talib.SMA(kagi_df[\"kagi_start\"], timeperiod=10)\n",
    "    kagi_df[\"kagi_sma20\"] = talib.SMA(kagi_df[\"kagi_start\"], timeperiod=20)\n",
    "\n",
    "    # Calculate Bollinger Bands\n",
    "    kagi_df[\"kagi_upper\"], kagi_df[\"kagi_middle\"], kagi_df[\"kagi_lower\"] = talib.BBANDS(\n",
    "        kagi_df[\"kagi_start\"], timeperiod=5, nbdevup=2, nbdevdn=2, matype=0\n",
    "    )\n",
    "\n",
    "    # Calculate MACD\n",
    "    (\n",
    "        kagi_df[\"kagi_macd\"],\n",
    "        kagi_df[\"kagi_macdsignal\"],\n",
    "        kagi_df[\"kagi_macdhist\"],\n",
    "    ) = talib.MACD(kagi_df[\"kagi_start\"], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "    # Calculate RSI\n",
    "    kagi_df[\"kagi_rsi\"] = talib.RSI(kagi_df[\"kagi_start\"], timeperiod=14)\n",
    "\n",
    "    # Calculate Stochastic Oscillator\n",
    "    kagi_df[\"kagi_slowk\"], kagi_df[\"kagi_slowd\"] = talib.STOCH(\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        5,\n",
    "        3,\n",
    "        0,\n",
    "        3,\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    kagi_df[\"kagi_atr\"] = talib.ATR(\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        timeperiod=14,\n",
    "    )\n",
    "\n",
    "    kagi_df[\"kagi_adx\"] = talib.ADX(\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        timeperiod=14,\n",
    "    )\n",
    "\n",
    "    # Convert df.index to Unix timestamp\n",
    "    df.index = df.index.astype(np.int64) // 10**9\n",
    "\n",
    "    # Align the volume series with kagi_df\n",
    "    aligned_volume = df[\"volume\"].reindex(kagi_df[\"time_unix\"], method=\"pad\")\n",
    "\n",
    "    # Calculate Accumulation / Distribution Line\n",
    "    kagi_df[\"kagi_ad\"] = talib.AD(\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        kagi_df[\"kagi_start\"],\n",
    "        aligned_volume,\n",
    "    )\n",
    "\n",
    "    return kagi_df\n",
    "\n",
    "\n",
    "def three_line_break(df):\n",
    "    df_tlb = pd.DataFrame()\n",
    "    df_tlb[\"open\"] = df[\"open\"].reset_index(drop=True)\n",
    "    df_tlb[\"close\"] = df[\"close\"].reset_index(drop=True)\n",
    "    df_tlb[\"high\"] = df[\"high\"].reset_index(drop=True)\n",
    "    df_tlb[\"low\"] = df[\"low\"].reset_index(drop=True)\n",
    "    df_tlb[\"volume\"] = df[\"volume\"].reset_index(drop=True)\n",
    "    df_tlb[\"tlb_direction\"] = None\n",
    "    df_tlb[\"time_unix\"] = df[\n",
    "        \"time_unix\"\n",
    "    ].values  # Copy 'time_unix' from original dataframe\n",
    "\n",
    "    if df_tlb[\"close\"][1] > df_tlb[\"close\"][0]:\n",
    "        df_tlb.loc[1, \"tlb_direction\"] = \"up\"\n",
    "    else:\n",
    "        df_tlb.loc[1, \"tlb_direction\"] = \"down\"\n",
    "\n",
    "    for i in range(2, len(df_tlb)):\n",
    "        up_condition = (df_tlb[\"close\"][i] > df_tlb[\"close\"][i - 3 : i].max()) and (\n",
    "            df_tlb[\"tlb_direction\"][i - 1] == \"down\"\n",
    "        )\n",
    "        down_condition = (df_tlb[\"close\"][i] < df_tlb[\"close\"][i - 3 : i].min()) and (\n",
    "            df_tlb[\"tlb_direction\"][i - 1] == \"up\"\n",
    "        )\n",
    "\n",
    "        if up_condition:\n",
    "            df_tlb.loc[i, \"tlb_direction\"] = \"up\"\n",
    "        elif down_condition:\n",
    "            df_tlb.loc[i, \"tlb_direction\"] = \"down\"\n",
    "        else:\n",
    "            df_tlb.loc[i, \"tlb_direction\"] = df_tlb.loc[i - 1, \"tlb_direction\"]\n",
    "\n",
    "    return df_tlb\n",
    "\n",
    "\n",
    "def add_tlb_features(df):\n",
    "    # Add Relative Strength Index (RSI) of the close price\n",
    "    df[\"tlb_RSI\"] = talib.RSI(df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Moving Average Convergence Divergence (MACD) of the close price\n",
    "    df[\"tlb_MACD\"], df[\"tlb_MACD_signal\"], df[\"tlb_MACD_hist\"] = talib.MACD(\n",
    "        df[\"close\"], fastperiod=12, slowperiod=26, signalperiod=9\n",
    "    )\n",
    "\n",
    "    # Add Bollinger Bands of the close price\n",
    "    df[\"tlb_upper_band\"], df[\"tlb_middle_band\"], df[\"tlb_lower_band\"] = talib.BBANDS(\n",
    "        df[\"close\"], timeperiod=20\n",
    "    )\n",
    "\n",
    "    # Add Simple Moving Average (SMA) of the close price\n",
    "    df[\"tlb_SMA\"] = talib.SMA(df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Exponential Moving Average (EMA) of the close price\n",
    "    df[\"tlb_EMA\"] = talib.EMA(df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Rate of Change (ROC) of the close price\n",
    "    df[\"tlb_ROC\"] = talib.ROC(df[\"close\"], timeperiod=10)\n",
    "\n",
    "    # Add Average True Range (ATR) of the close price\n",
    "    df[\"tlb_ATR\"] = talib.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Momentum of the close price\n",
    "    df[\"tlb_momentum\"] = talib.MOM(df[\"close\"], timeperiod=10)\n",
    "\n",
    "    # Add Stochastic Oscillator %K and %D of the close price\n",
    "    df[\"tlb_slowk\"], df[\"tlb_slowd\"] = talib.STOCH(\n",
    "        df[\"high\"],\n",
    "        df[\"low\"],\n",
    "        df[\"close\"],\n",
    "        fastk_period=5,\n",
    "        slowk_period=3,\n",
    "        slowk_matype=0,\n",
    "        slowd_period=3,\n",
    "        slowd_matype=0,\n",
    "    )\n",
    "\n",
    "    # Add Commodity Channel Index (CCI) of the close price\n",
    "    df[\"tlb_CCI\"] = talib.CCI(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add On Balance Volume (OBV)\n",
    "    df[\"tlb_OBV\"] = talib.OBV(df[\"close\"], df[\"volume\"])\n",
    "\n",
    "    # Add Moving Average of High, Low, Open prices\n",
    "    df[\"tlb_MA_high\"] = talib.MA(df[\"high\"], timeperiod=14)\n",
    "    df[\"tlb_MA_low\"] = talib.MA(df[\"low\"], timeperiod=14)\n",
    "    df[\"tlb_MA_open\"] = talib.MA(df[\"open\"], timeperiod=14)\n",
    "\n",
    "    # Add Historical volatility\n",
    "    df[\"tlb_volatility\"] = (\n",
    "        talib.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14) / df[\"tlb_SMA\"]\n",
    "    )\n",
    "\n",
    "    # Add Money Flow Index (MFI)\n",
    "    df[\"tlb_MFI\"] = talib.MFI(\n",
    "        df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"], timeperiod=14\n",
    "    )\n",
    "\n",
    "    # Add Chaikin Money Flow (CMF)\n",
    "    df[\"tlb_CMF\"] = (\n",
    "        df[\"close\"] - df[\"low\"] - (df[\"high\"] - df[\"close\"]) / (df[\"high\"] - df[\"low\"])\n",
    "    ) * df[\"volume\"]\n",
    "\n",
    "    # Add William’s %R\n",
    "    df[\"tlb_WilliamsR\"] = talib.WILLR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # Add Ultimate Oscillator\n",
    "    df[\"tlb_UO\"] = talib.ULTOSC(\n",
    "        df[\"high\"],\n",
    "        df[\"low\"],\n",
    "        df[\"close\"],\n",
    "        timeperiod1=7,\n",
    "        timeperiod2=14,\n",
    "        timeperiod3=28,\n",
    "    )\n",
    "\n",
    "    # Add Accumulation/Distribution Line (ADL)\n",
    "    df[\"tlb_ADL\"] = talib.AD(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"])\n",
    "\n",
    "    # Add Average Directional Index (ADX)\n",
    "    df[\"tlb_ADX\"] = talib.ADX(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def point_and_figure(df, box_size):\n",
    "    # Create a copy of the data frame\n",
    "    df_pnf = df.copy()\n",
    "\n",
    "    # Calculate column direction\n",
    "    df_pnf[\"direction\"] = (\n",
    "        df_pnf[\"close\"]\n",
    "        .diff()\n",
    "        .apply(lambda x: 1 if x > box_size else 0 if x < -box_size else np.nan)\n",
    "    )\n",
    "\n",
    "    # Initialize the first row's Direction\n",
    "    if len(df_pnf) > 0:\n",
    "        df_pnf.at[df_pnf.index[0], \"direction\"] = (\n",
    "            1 if df_pnf[\"close\"].iloc[0] - df_pnf[\"open\"].iloc[0] >= box_size else 0\n",
    "        )\n",
    "\n",
    "    # Forward fill direction column\n",
    "    df_pnf[\"direction\"] = df_pnf[\"direction\"].fillna(method=\"ffill\")\n",
    "\n",
    "    # Calculate price changes\n",
    "    df_pnf[\"change\"] = (\n",
    "        df_pnf[\"close\"].diff().apply(lambda x: np.nan if abs(x) < box_size else x)\n",
    "    )\n",
    "\n",
    "    # Drop rows without price change\n",
    "    df_pnf = df_pnf.dropna()\n",
    "\n",
    "    # Create the new columns for PnF values\n",
    "    df_pnf[\"pnf_open\"] = df_pnf[\"open\"]\n",
    "    df_pnf[\"pnf_close\"] = df_pnf[\"close\"]\n",
    "    df_pnf[\"pnf_high\"] = df_pnf[\"high\"]\n",
    "    df_pnf[\"pnf_low\"] = df_pnf[\"low\"]\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_pnf = df_pnf.drop_duplicates()\n",
    "\n",
    "    return df_pnf\n",
    "\n",
    "\n",
    "def add_pnf_features(df):\n",
    "    # Calculate difference between open and close prices\n",
    "    df[\"pnf_O-C\"] = df[\"pnf_open\"] - df[\"pnf_close\"]\n",
    "\n",
    "    # Calculate difference between high and low prices\n",
    "    df[\"pnf_H-L\"] = df[\"pnf_high\"] - df[\"pnf_low\"]\n",
    "\n",
    "    # Calculate difference between high and open prices\n",
    "    df[\"pnf_H-O\"] = df[\"pnf_high\"] - df[\"pnf_open\"]\n",
    "\n",
    "    # Calculate difference between low and close prices\n",
    "    df[\"pnf_L-C\"] = df[\"pnf_low\"] - df[\"pnf_close\"]\n",
    "\n",
    "    # # Calculate moving average\n",
    "    # df[\"pnf_MA\"] = talib.MA(df[\"close\"], timeperiod=20)\n",
    "\n",
    "    # # Calculate exponential moving average\n",
    "    # df[\"pnf_EMA\"] = talib.EMA(df[\"close\"], timeperiod=20)\n",
    "\n",
    "    # # Calculate RSI\n",
    "    # df[\"pnf_RSI\"] = talib.RSI(df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # # Calculate MACD\n",
    "    # MACD_line, signal_line, hist = talib.MACD(\n",
    "    #     df[\"close\"], fastperiod=12, slowperiod=26, signalperiod=9\n",
    "    # )\n",
    "    # df[\"pnf_MACD\"] = MACD_line - signal_line\n",
    "\n",
    "    # # Calculate Stochastic\n",
    "    # slowk, slowd = talib.STOCH(\n",
    "    #     df[\"high\"],\n",
    "    #     df[\"low\"],\n",
    "    #     df[\"close\"],\n",
    "    #     fastk_period=14,\n",
    "    #     slowk_period=3,\n",
    "    #     slowk_matype=0,\n",
    "    #     slowd_period=3,\n",
    "    #     slowd_matype=0,\n",
    "    # )\n",
    "    # df[\"pnf_Stochastic\"] = slowk\n",
    "\n",
    "    # # Calculate Bollinger Bands\n",
    "    # upper, middle, lower = talib.BBANDS(df[\"close\"], timeperiod=20)\n",
    "    # df[\"pnf_UpperBB\"] = upper\n",
    "    # df[\"pnf_MiddleBB\"] = middle\n",
    "    # df[\"pnf_LowerBB\"] = lower\n",
    "\n",
    "    # # Calculate ADX\n",
    "    # df[\"pnf_ADX\"] = talib.ADX(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # # Calculate CCI\n",
    "    # df[\"pnf_CCI\"] = talib.CCI(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    # # Calculate ROC\n",
    "    # df[\"pnf_ROC\"] = talib.ROC(df[\"close\"], timeperiod=10)\n",
    "\n",
    "    # # Calculate ATR\n",
    "    # df[\"pnf_ATR\"] = talib.ATR(df[\"high\"], df[\"low\"], df[\"close\"], timeperiod=14)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_features(df):\n",
    "    ## DIFFERENCES ##\n",
    "\n",
    "    # Price differences\n",
    "    df[\"price_diff\"] = df[\"close\"].diff()\n",
    "    df[\"op_cl_diff\"] = df[\"open\"] - df[\"close\"]\n",
    "\n",
    "    # Moving averages\n",
    "    df[\"ma_5\"] = df[\"close\"].rolling(window=5).mean()\n",
    "    df[\"ma_10\"] = df[\"close\"].rolling(window=10).mean()\n",
    "\n",
    "    # Price percentage change\n",
    "    df[\"pct_change\"] = df[\"close\"].pct_change()\n",
    "\n",
    "    # RSI\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df[\"rsi\"] = 100 - 100 / (1 + rs)\n",
    "\n",
    "    # Other popular difference features\n",
    "    for i in range(1, 35):\n",
    "        df[\"diff_{}\".format(i)] = df[\"close\"].diff(i)\n",
    "\n",
    "    sma = df[\"close\"].rolling(window=20).mean()\n",
    "    std = df[\"close\"].rolling(window=20).std()\n",
    "    df[\"upper_band\"] = sma + (2 * std)\n",
    "    df[\"lower_band\"] = sma - (2 * std)\n",
    "\n",
    "    highest_high = df[\"high\"].rolling(window=14).max()\n",
    "    lowest_low = df[\"low\"].rolling(window=14).min()\n",
    "    df[\"williams_r\"] = (highest_high - df[\"close\"]) / (highest_high - lowest_low) * -100\n",
    "\n",
    "    df[\"obv\"] = (np.sign(df[\"close\"].diff()) * df[\"volume\"]).fillna(0).cumsum()\n",
    "\n",
    "    tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3\n",
    "    sma = tp.rolling(window=20).mean()\n",
    "    mean_deviation = tp.rolling(window=20).apply(\n",
    "        lambda x: np.mean(np.abs(x - x.mean()))\n",
    "    )\n",
    "    df[\"cci\"] = (tp - sma) / (0.015 * mean_deviation)\n",
    "\n",
    "    true_range = pd.concat(\n",
    "        [\n",
    "            df[\"high\"] - df[\"low\"],\n",
    "            (df[\"high\"] - df[\"close\"].shift()).abs(),\n",
    "            (df[\"close\"].shift() - df[\"low\"]).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1)\n",
    "    df[\"atr\"] = true_range.rolling(window=14).mean()\n",
    "\n",
    "    money_flow_vol = (\n",
    "        ((df[\"close\"] - df[\"low\"]) - (df[\"high\"] - df[\"close\"]))\n",
    "        / (df[\"high\"] - df[\"low\"])\n",
    "        * df[\"volume\"]\n",
    "    )\n",
    "    cmf = (\n",
    "        money_flow_vol.rolling(window=20).sum() / df[\"volume\"].rolling(window=20).sum()\n",
    "    )\n",
    "    df[\"cmf\"] = cmf\n",
    "\n",
    "    ema_12 = df[\"close\"].ewm(span=12).mean()\n",
    "    ema_26 = df[\"close\"].ewm(span=26).mean()\n",
    "    ppo = (ema_12 - ema_26) / ema_26 * 100\n",
    "    df[\"ppo\"] = ppo\n",
    "\n",
    "    ### LAG ###\n",
    "\n",
    "    # Lag closing prices\n",
    "    for i in range(1, 11):\n",
    "        df[\"lag_close_{}\".format(i)] = df[\"close\"].shift(i)\n",
    "\n",
    "    # Lag daily returns\n",
    "    returns = df[\"close\"].pct_change()\n",
    "    for i in range(1, 6):\n",
    "        df[\"lag_return_{}\".format(i)] = returns.shift(i)\n",
    "\n",
    "    # Lag high and low prices\n",
    "    for i in range(1, 4):\n",
    "        df[\"lag_high_{}\".format(i)] = df[\"high\"].shift(i)\n",
    "        df[\"lag_low_{}\".format(i)] = df[\"low\"].shift(i)\n",
    "\n",
    "    # Historical volatility\n",
    "    df[\"hist_volatility_10\"] = returns.rolling(window=10).std()\n",
    "    df[\"hist_volatility_20\"] = returns.rolling(window=20).std()\n",
    "    df[\"hist_volatility_30\"] = returns.rolling(window=30).std()\n",
    "\n",
    "    # Previous day's RSI value\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - 100 / (1 + rs)\n",
    "    df[\"lag_rsi_1\"] = rsi.shift(1)\n",
    "\n",
    "    ### ROLLING ###\n",
    "\n",
    "    # Moving averages\n",
    "    df[\"ma_5\"] = df[\"close\"].rolling(window=5).mean()\n",
    "    df[\"ma_10\"] = df[\"close\"].rolling(window=10).mean()\n",
    "    df[\"ma_20\"] = df[\"close\"].rolling(window=20).mean()\n",
    "\n",
    "    # Exponential moving averages\n",
    "    df[\"ema_5\"] = df[\"close\"].ewm(span=5).mean()\n",
    "    df[\"ema_10\"] = df[\"close\"].ewm(span=10).mean()\n",
    "    df[\"ema_20\"] = df[\"close\"].ewm(span=20).mean()\n",
    "\n",
    "    # Bollinger Bands\n",
    "    sma = df[\"close\"].rolling(window=20).mean()\n",
    "    std = df[\"close\"].rolling(window=20).std()\n",
    "    df[\"upper_band\"] = sma + (2 * std)\n",
    "    df[\"lower_band\"] = sma - (2 * std)\n",
    "\n",
    "    # Rate of Change (ROC)\n",
    "    df[\"roc_5\"] = df[\"close\"].pct_change(periods=5)\n",
    "    df[\"roc_10\"] = df[\"close\"].pct_change(periods=10)\n",
    "\n",
    "    # Standard deviation\n",
    "    df[\"std_5\"] = df[\"close\"].rolling(window=5).std()\n",
    "    df[\"std_10\"] = df[\"close\"].rolling(window=10).std()\n",
    "\n",
    "    # Average True Range (ATR)\n",
    "    true_range = pd.concat(\n",
    "        [\n",
    "            df[\"high\"] - df[\"low\"],\n",
    "            (df[\"high\"] - df[\"close\"].shift()).abs(),\n",
    "            (df[\"low\"] - df[\"close\"].shift()).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1)\n",
    "    df[\"atr_14\"] = true_range.rolling(window=14).mean()\n",
    "\n",
    "    # Keltner Channels\n",
    "    middle_line = df[\"close\"].rolling(window=20).mean()\n",
    "    upper_keltner = middle_line + 2 * df[\"atr_14\"]\n",
    "    lower_keltner = middle_line - 2 * df[\"atr_14\"]\n",
    "    df[\"upper_keltner\"] = upper_keltner\n",
    "    df[\"lower_keltner\"] = lower_keltner\n",
    "\n",
    "    # RSI\n",
    "    delta = df[\"close\"].diff()\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    df[\"rsi\"] = 100 - 100 / (1 + rs)\n",
    "\n",
    "    ### RATIO ###\n",
    "\n",
    "    # Calculate some basic features first\n",
    "    returns = df[\"close\"].pct_change()\n",
    "    ema_12 = df[\"close\"].ewm(span=12).mean()\n",
    "    ema_26 = df[\"close\"].ewm(span=26).mean()\n",
    "\n",
    "    # 1. Price-to-moving-average ratios\n",
    "    df[\"price_to_ma_5\"] = df[\"close\"] / df[\"ma_5\"]\n",
    "    df[\"price_to_ma_10\"] = df[\"close\"] / df[\"ma_10\"]\n",
    "    df[\"price_to_ma_20\"] = df[\"close\"] / df[\"ma_20\"]\n",
    "\n",
    "    # 2. Price-to-EMA ratios\n",
    "    df[\"price_to_ema_5\"] = df[\"close\"] / df[\"ema_5\"]\n",
    "    df[\"price_to_ema_10\"] = df[\"close\"] / df[\"ema_10\"]\n",
    "    df[\"price_to_ema_20\"] = df[\"close\"] / df[\"ema_20\"]\n",
    "\n",
    "    # 3. EMA-to-moving-average ratios\n",
    "    df[\"ema_to_ma_5\"] = df[\"ema_5\"] / df[\"ma_5\"]\n",
    "    df[\"ema_to_ma_10\"] = df[\"ema_10\"] / df[\"ma_10\"]\n",
    "    df[\"ema_to_ma_20\"] = df[\"ema_20\"] / df[\"ma_20\"]\n",
    "\n",
    "    # 4. EMA-to-EMA ratios\n",
    "    df[\"ema5_to_ema10\"] = df[\"ema_5\"] / df[\"ema_10\"]\n",
    "    df[\"ema5_to_ema20\"] = df[\"ema_5\"] / df[\"ema_20\"]\n",
    "    df[\"ema10_to_ema20\"] = df[\"ema_10\"] / df[\"ema_20\"]\n",
    "\n",
    "    # 5. Moving-average-to-moving-average ratios\n",
    "    df[\"ma5_to_ma10\"] = df[\"ma_5\"] / df[\"ma_10\"]\n",
    "    df[\"ma5_to_ma20\"] = df[\"ma_5\"] / df[\"ma_20\"]\n",
    "    df[\"ma10_to_ma20\"] = df[\"ma_10\"] / df[\"ma_20\"]\n",
    "\n",
    "    # 6. Price-to-Bollinger-Band ratios\n",
    "    df[\"price_to_upper_band\"] = df[\"close\"] / df[\"upper_band\"]\n",
    "    df[\"price_to_lower_band\"] = df[\"close\"] / df[\"lower_band\"]\n",
    "\n",
    "    # 7. Price-to-Keltner-Channel ratios\n",
    "    df[\"price_to_upper_keltner\"] = df[\"close\"] / df[\"upper_keltner\"]\n",
    "    df[\"price_to_lower_keltner\"] = df[\"close\"] / df[\"lower_keltner\"]\n",
    "\n",
    "    # 8. Price-to-previous-day-high ratio\n",
    "    df[\"price_to_prev_high\"] = df[\"close\"] / df[\"high\"].shift(1)\n",
    "\n",
    "    # 9. Price-to-previous-day-low ratio\n",
    "    df[\"price_to_prev_low\"] = df[\"close\"] / df[\"low\"].shift(1)\n",
    "\n",
    "    # 10. RSI-to-moving-average ratio\n",
    "    df[\"rsi_to_ma_5\"] = df[\"rsi\"] / df[\"ma_5\"]\n",
    "\n",
    "    ### CORRELATION ####\n",
    "\n",
    "    # Calculate some basic features first\n",
    "    returns = df[\"close\"].pct_change()\n",
    "    volume_change = df[\"volume\"].pct_change()\n",
    "\n",
    "    # 1. Correlation with returns and volume (short-term)\n",
    "    df[\"return_volume_corr_5\"] = returns.rolling(window=5).corr(df[\"volume\"])\n",
    "    df[\"return_volume_corr_10\"] = returns.rolling(window=10).corr(df[\"volume\"])\n",
    "    df[\"return_volume_corr_20\"] = returns.rolling(window=20).corr(df[\"volume\"])\n",
    "\n",
    "    # 2. Correlation with returns and volume (medium-term)\n",
    "    df[\"return_volume_corr_50\"] = returns.rolling(window=50).corr(df[\"volume\"])\n",
    "    df[\"return_volume_corr_100\"] = returns.rolling(window=100).corr(df[\"volume\"])\n",
    "\n",
    "    # 3. Correlation with price and volume\n",
    "    df[\"price_volume_corr_5\"] = df[\"close\"].rolling(window=5).corr(df[\"volume\"])\n",
    "    df[\"price_volume_corr_10\"] = df[\"close\"].rolling(window=10).corr(df[\"volume\"])\n",
    "    df[\"price_volume_corr_20\"] = df[\"close\"].rolling(window=20).corr(df[\"volume\"])\n",
    "\n",
    "    # 4. Correlation with price and volume (medium-term)\n",
    "    df[\"price_volume_corr_50\"] = df[\"close\"].rolling(window=50).corr(df[\"volume\"])\n",
    "    df[\"price_volume_corr_100\"] = df[\"close\"].rolling(window=100).corr(df[\"volume\"])\n",
    "\n",
    "    # 5. Price / volume\n",
    "    df[\"price_to_volume\"] = df[\"close\"] / df[\"volume\"]\n",
    "\n",
    "    # 6. Volume Relative Strength Index (VRSI)\n",
    "    delta_vol = df[\"volume\"].diff()\n",
    "    gain_vol = delta_vol.where(delta_vol > 0, 0)\n",
    "    loss_vol = -delta_vol.where(delta_vol < 0, 0)\n",
    "    avg_gain_vol = gain_vol.rolling(window=14).mean()\n",
    "    avg_loss_vol = loss_vol.rolling(window=14).mean()\n",
    "    rs_vol = avg_gain_vol / avg_loss_vol\n",
    "    df[\"vrsi\"] = 100 - 100 / (1 + rs_vol)\n",
    "\n",
    "    # 7. Change in volume\n",
    "    df[\"volume_change\"] = volume_change\n",
    "\n",
    "    # 8. Moving averages of volume\n",
    "    df[\"volume_ma_5\"] = df[\"volume\"].rolling(window=5).mean()\n",
    "    df[\"volume_ma_10\"] = df[\"volume\"].rolling(window=10).mean()\n",
    "    df[\"volume_ma_20\"] = df[\"volume\"].rolling(window=20).mean()\n",
    "\n",
    "    # 9. Standard deviation of volume\n",
    "    df[\"volume_std_5\"] = df[\"volume\"].rolling(window=5).std()\n",
    "    df[\"volume_std_10\"] = df[\"volume\"].rolling(window=10).std()\n",
    "    df[\"volume_std_20\"] = df[\"volume\"].rolling(window=20).std()\n",
    "\n",
    "    # 10. Ratio of volume to moving average volume\n",
    "    df[\"volume_to_ma_volume\"] = df[\"volume\"] / df[\"volume_ma_20\"]\n",
    "\n",
    "    ### EXTRA ###\n",
    "\n",
    "    # Calculate necessary base features\n",
    "    returns = df[\"close\"].pct_change()\n",
    "    delta = df[\"close\"].diff()\n",
    "\n",
    "    # close_change\n",
    "    df[\"close_change\"] = df[\"close\"].diff()\n",
    "\n",
    "    # high_pct\n",
    "    df[\"high_pct\"] = df[\"high\"].pct_change()\n",
    "\n",
    "    # close_change_roll5\n",
    "    df[\"close_change_roll5\"] = df[\"close_change\"].rolling(window=5).mean()\n",
    "\n",
    "    # RSI_14_roll5\n",
    "    gain = delta.where(delta > 0, 0)\n",
    "    loss = -delta.where(delta < 0, 0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - 100 / (1 + rs)\n",
    "    df[\"RSI_14_roll5\"] = rsi.rolling(window=5).mean()\n",
    "\n",
    "    # ATR_14_roll5\n",
    "    true_range = pd.concat(\n",
    "        [\n",
    "            df[\"high\"] - df[\"low\"],\n",
    "            (df[\"high\"] - df[\"close\"].shift()).abs(),\n",
    "            (df[\"low\"] - df[\"close\"].shift()).abs(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ).max(axis=1)\n",
    "    df[\"ATR_14_roll5\"] = true_range.rolling(window=14).mean().rolling(window=5).mean()\n",
    "\n",
    "    # volume_roll5\n",
    "    df[\"volume_roll5\"] = df[\"volume\"].rolling(window=5).mean()\n",
    "\n",
    "    # high_pct_roll5\n",
    "    df[\"high_pct_roll5\"] = df[\"high_pct\"].rolling(window=5).mean()\n",
    "\n",
    "    # volatility_5\n",
    "    df[\"volatility_5\"] = df[\"close\"].rolling(window=5).std()\n",
    "\n",
    "    # price_ema5\n",
    "    df[\"price_ema5\"] = df[\"close\"].ewm(span=5).mean()\n",
    "\n",
    "    # volume_ema5\n",
    "    df[\"volume_ema5\"] = df[\"volume\"].ewm(span=5).mean()\n",
    "\n",
    "    # price_to_ema5\n",
    "    df[\"price_to_ema5\"] = df[\"close\"] / df[\"price_ema5\"]\n",
    "\n",
    "    # volume_change_roll5\n",
    "    df[\"volume_change_roll5\"] = volume_change.rolling(window=5).mean()\n",
    "\n",
    "    # avg_vol_last_100\n",
    "    df[\"avg_vol_last_100\"] = df[\"volume\"].rolling(window=100).mean()\n",
    "\n",
    "    # turnover\n",
    "    df[\"turnover\"] = df[\"volume\"] * df[\"close\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def timeseries_cv_score(X, y, n_splits):\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    f1_scores = []\n",
    "    auc_scores = []  # list to store ROC AUC scores for each split\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Define LSTM model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))  # because of binary classification\n",
    "\n",
    "        model.compile(\n",
    "            loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = model.predict(X_test).ravel()\n",
    "\n",
    "        # Calculate F1 score of the model on the test set\n",
    "        f1 = f1_score(y_test, (y_pred > 0.5).astype(\"int32\"))\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "        # Calculate ROC AUC score of the model on the test set\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "    return np.mean(f1_scores), np.mean(auc_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to show all rows and columns\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Prepare TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "# Load the data\n",
    "data_path = \"../../../data/kc/btc/raw/kc_btc_15min.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>color</th>\n",
       "      <th>color_change</th>\n",
       "      <th>time_unix</th>\n",
       "      <th>direction</th>\n",
       "      <th>change</th>\n",
       "      <th>pnf_open</th>\n",
       "      <th>pnf_close</th>\n",
       "      <th>pnf_high</th>\n",
       "      <th>pnf_low</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-03-27 09:00:00</th>\n",
       "      <td>27827.6</td>\n",
       "      <td>27866.9</td>\n",
       "      <td>27869.0</td>\n",
       "      <td>27778.3</td>\n",
       "      <td>32.308052</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1679907600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.3</td>\n",
       "      <td>27827.6</td>\n",
       "      <td>27866.9</td>\n",
       "      <td>27869.0</td>\n",
       "      <td>27778.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 09:15:00</th>\n",
       "      <td>27866.9</td>\n",
       "      <td>27848.9</td>\n",
       "      <td>27873.4</td>\n",
       "      <td>27839.5</td>\n",
       "      <td>21.288271</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1679908500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>27866.9</td>\n",
       "      <td>27848.9</td>\n",
       "      <td>27873.4</td>\n",
       "      <td>27839.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 09:30:00</th>\n",
       "      <td>27848.7</td>\n",
       "      <td>27852.0</td>\n",
       "      <td>27867.8</td>\n",
       "      <td>27814.5</td>\n",
       "      <td>31.791354</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1679909400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>27848.7</td>\n",
       "      <td>27852.0</td>\n",
       "      <td>27867.8</td>\n",
       "      <td>27814.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 09:45:00</th>\n",
       "      <td>27852.0</td>\n",
       "      <td>27882.2</td>\n",
       "      <td>27893.9</td>\n",
       "      <td>27838.1</td>\n",
       "      <td>21.251086</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1679910300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.2</td>\n",
       "      <td>27852.0</td>\n",
       "      <td>27882.2</td>\n",
       "      <td>27893.9</td>\n",
       "      <td>27838.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 10:00:00</th>\n",
       "      <td>27882.1</td>\n",
       "      <td>27888.3</td>\n",
       "      <td>27902.9</td>\n",
       "      <td>27863.4</td>\n",
       "      <td>21.638241</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1679911200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>27882.1</td>\n",
       "      <td>27888.3</td>\n",
       "      <td>27902.9</td>\n",
       "      <td>27863.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 10:15:00</th>\n",
       "      <td>27888.2</td>\n",
       "      <td>27839.8</td>\n",
       "      <td>27888.3</td>\n",
       "      <td>27839.7</td>\n",
       "      <td>25.172174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1679912100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-48.5</td>\n",
       "      <td>27888.2</td>\n",
       "      <td>27839.8</td>\n",
       "      <td>27888.3</td>\n",
       "      <td>27839.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 10:30:00</th>\n",
       "      <td>27839.8</td>\n",
       "      <td>27887.9</td>\n",
       "      <td>27901.0</td>\n",
       "      <td>27828.4</td>\n",
       "      <td>23.995132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1679913000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>48.1</td>\n",
       "      <td>27839.8</td>\n",
       "      <td>27887.9</td>\n",
       "      <td>27901.0</td>\n",
       "      <td>27828.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 11:00:00</th>\n",
       "      <td>27889.6</td>\n",
       "      <td>27930.3</td>\n",
       "      <td>27930.9</td>\n",
       "      <td>27872.4</td>\n",
       "      <td>29.465377</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1679914800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.8</td>\n",
       "      <td>27889.6</td>\n",
       "      <td>27930.3</td>\n",
       "      <td>27930.9</td>\n",
       "      <td>27872.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 11:30:00</th>\n",
       "      <td>27930.0</td>\n",
       "      <td>27889.2</td>\n",
       "      <td>27930.0</td>\n",
       "      <td>27889.1</td>\n",
       "      <td>18.444079</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1679916600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.9</td>\n",
       "      <td>27930.0</td>\n",
       "      <td>27889.2</td>\n",
       "      <td>27930.0</td>\n",
       "      <td>27889.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-27 11:45:00</th>\n",
       "      <td>27889.2</td>\n",
       "      <td>27903.4</td>\n",
       "      <td>27916.6</td>\n",
       "      <td>27874.6</td>\n",
       "      <td>14.993546</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1679917500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>27889.2</td>\n",
       "      <td>27903.4</td>\n",
       "      <td>27916.6</td>\n",
       "      <td>27874.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        open    close     high      low     volume  color  \\\n",
       "time                                                                        \n",
       "2023-03-27 09:00:00  27827.6  27866.9  27869.0  27778.3  32.308052      1   \n",
       "2023-03-27 09:15:00  27866.9  27848.9  27873.4  27839.5  21.288271      0   \n",
       "2023-03-27 09:30:00  27848.7  27852.0  27867.8  27814.5  31.791354      1   \n",
       "2023-03-27 09:45:00  27852.0  27882.2  27893.9  27838.1  21.251086      1   \n",
       "2023-03-27 10:00:00  27882.1  27888.3  27902.9  27863.4  21.638241      1   \n",
       "2023-03-27 10:15:00  27888.2  27839.8  27888.3  27839.7  25.172174      0   \n",
       "2023-03-27 10:30:00  27839.8  27887.9  27901.0  27828.4  23.995132      1   \n",
       "2023-03-27 11:00:00  27889.6  27930.3  27930.9  27872.4  29.465377      1   \n",
       "2023-03-27 11:30:00  27930.0  27889.2  27930.0  27889.1  18.444079      0   \n",
       "2023-03-27 11:45:00  27889.2  27903.4  27916.6  27874.6  14.993546      1   \n",
       "\n",
       "                     color_change   time_unix  direction  change  pnf_open  \\\n",
       "time                                                                         \n",
       "2023-03-27 09:00:00             1  1679907600        1.0    39.3   27827.6   \n",
       "2023-03-27 09:15:00             1  1679908500        0.0   -18.0   27866.9   \n",
       "2023-03-27 09:30:00             1  1679909400        1.0     3.1   27848.7   \n",
       "2023-03-27 09:45:00             0  1679910300        1.0    30.2   27852.0   \n",
       "2023-03-27 10:00:00             0  1679911200        1.0     6.1   27882.1   \n",
       "2023-03-27 10:15:00             1  1679912100        0.0   -48.5   27888.2   \n",
       "2023-03-27 10:30:00             1  1679913000        1.0    48.1   27839.8   \n",
       "2023-03-27 11:00:00             1  1679914800        1.0    40.8   27889.6   \n",
       "2023-03-27 11:30:00             0  1679916600        0.0   -40.9   27930.0   \n",
       "2023-03-27 11:45:00             1  1679917500        1.0    14.2   27889.2   \n",
       "\n",
       "                     pnf_close  pnf_high  pnf_low  \n",
       "time                                               \n",
       "2023-03-27 09:00:00    27866.9   27869.0  27778.3  \n",
       "2023-03-27 09:15:00    27848.9   27873.4  27839.5  \n",
       "2023-03-27 09:30:00    27852.0   27867.8  27814.5  \n",
       "2023-03-27 09:45:00    27882.2   27893.9  27838.1  \n",
       "2023-03-27 10:00:00    27888.3   27902.9  27863.4  \n",
       "2023-03-27 10:15:00    27839.8   27888.3  27839.7  \n",
       "2023-03-27 10:30:00    27887.9   27901.0  27828.4  \n",
       "2023-03-27 11:00:00    27930.3   27930.9  27872.4  \n",
       "2023-03-27 11:30:00    27889.2   27930.0  27889.1  \n",
       "2023-03-27 11:45:00    27903.4   27916.6  27874.6  "
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "df = load_data(data_path)\n",
    "df = preprocess_data(df)\n",
    "\n",
    "# print(df.columns)\n",
    "\n",
    "# Timestamp conversion and index setting\n",
    "df[\"time_unix\"] = df[\"time\"]\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\")  # Assuming 'time' is in seconds\n",
    "df.set_index(\"time\", inplace=True)\n",
    "df = df.loc[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "df_pnf = point_and_figure(df, 3)\n",
    "df_pnf.tail(10)\n",
    "\n",
    "\n",
    "# # Technical Analysis features\n",
    "# df.ta.strategy(\"all\")\n",
    "\n",
    "# # Check your results and exclude as necessary.\n",
    "# df.ta.strategy(fast=10, slow=50, verbose=True)\n",
    "\n",
    "\n",
    "# # Heiken Ashi\n",
    "# df = add_heiken_ashi_features(df)\n",
    "\n",
    "# print(\"'time_unix' in df after Heiken Ashi features: \", \"time_unix\" in df.columns)\n",
    "# print(df[\"time_unix\"].head())\n",
    "# print(\"Rows in df after Heiken Ashi features: \", len(df))\n",
    "\n",
    "\n",
    "# ## RENKO\n",
    "# # CONSIDER CHANGING multiplyer to .5 or even 1\n",
    "# renko_df = compute_renko(df)\n",
    "\n",
    "# print(\"'time_unix' in renko_df: \", \"time_unix\" in renko_df.columns)\n",
    "# print(renko_df[\"time_unix\"].head())\n",
    "# print(\"Rows in renko_df: \", len(renko_df))\n",
    "\n",
    "# df = add_renko_features(df, renko_df)\n",
    "\n",
    "# print(\"'time_unix' in df after Renko features: \", \"time_unix\" in df.columns)\n",
    "# print(df[\"time_unix\"].head())\n",
    "# print(\"Rows in df after Renko features: \", len(df))\n",
    "\n",
    "# ## KAGI ##\n",
    "\n",
    "# kagi_df = kagi(df)\n",
    "# print(\"'time_unix' in kagi_df: \", \"time_unix\" in kagi_df.columns)\n",
    "# print(kagi_df[\"time_unix\"].head())\n",
    "# print(\"Rows in kagi_df: \", len(kagi_df))\n",
    "\n",
    "# # Add Kagi features\n",
    "# kagi_df = add_kagi_features(df, kagi_df)\n",
    "\n",
    "# print(\"'time_unix' in kagi_df after Kagi features: \", \"time_unix\" in kagi_df.columns)\n",
    "# print(kagi_df[\"time_unix\"].head())\n",
    "# print(\"Rows in kagi_df after Kagi features: \", len(kagi_df))\n",
    "\n",
    "# df = df.join(kagi_df.set_index(\"time_unix\"), on=\"time_unix\")\n",
    "\n",
    "# print(\"'time_unix' in df after joining with kagi_df: \", \"time_unix\" in df.columns)\n",
    "# print(df[\"time_unix\"].head())\n",
    "# print(\"Rows in df after joining with kagi_df: \", len(df))\n",
    "\n",
    "# ## Three Line Break ##\n",
    "\n",
    "# df_tlb = three_line_break(df)\n",
    "# direction_reversals = (\n",
    "#     df_tlb[\"tlb_direction\"].shift(1) != df_tlb[\"tlb_direction\"]\n",
    "# ).sum()\n",
    "# print(\"The number of direction reversals:\", direction_reversals)\n",
    "\n",
    "\n",
    "# # Create the TLB dataframe\n",
    "# tlb_df = three_line_break(df)\n",
    "# print(\"'time_unix' in tlb_df: \", \"time_unix\" in tlb_df.columns)\n",
    "# print(tlb_df[\"time_unix\"].head())\n",
    "# print(\"Rows in tlb_df: \", len(tlb_df))\n",
    "\n",
    "# # Add TLB features\n",
    "# tlb_df = add_tlb_features(tlb_df)\n",
    "\n",
    "# print(\"'time_unix' in tlb_df after TLB features: \", \"time_unix\" in tlb_df.columns)\n",
    "# print(tlb_df[\"time_unix\"].head())\n",
    "# print(\"Rows in tlb_df after TLB features: \", len(tlb_df))\n",
    "\n",
    "# # Join TLB dataframe with original dataframe\n",
    "# df = df.join(tlb_df.set_index(\"time_unix\"), on=\"time_unix\", rsuffix=\"_tlb\")\n",
    "\n",
    "# print(\"'time_unix' in df after joining with tlb_df: \", \"time_unix\" in df.columns)\n",
    "# print(df[\"time_unix\"].head())\n",
    "# print(\"Rows in df after joining with tlb_df: \", len(df))\n",
    "\n",
    "\n",
    "# print(df.columns)\n",
    "\n",
    "\n",
    "# # Additional features\n",
    "# df = create_features(df)\n",
    "\n",
    "# # Now extract additional date information from the original dataframe\n",
    "# df[\"minute\"] = df.index.minute\n",
    "# df[\"hour\"] = df.index.hour\n",
    "# df[\"day\"] = df.index.day\n",
    "# df[\"month\"] = df.index.month\n",
    "\n",
    "# # Sanity check. Make sure all the columns and types\n",
    "# # print(df.columns)\n",
    "\n",
    "# # Forward Fill\n",
    "# df.ffill(inplace=True)\n",
    "\n",
    "# # Backward Fill\n",
    "# df.bfill(inplace=True)\n",
    "\n",
    "# # # Finding problem features for standard scalar\n",
    "# # non_num_features = df.select_dtypes(exclude=[\"int32\", \"int64\", \"float32\", \"float64\"])\n",
    "# # print(\"Fix these features:\\n\")\n",
    "# # for col, dtype in non_num_features.dtypes.items():\n",
    "# #     print(f\"{col}: {dtype}\")\n",
    "\n",
    "\n",
    "# # Select numeric columns which need to be scaled\n",
    "# do_not_scale_columns = [\n",
    "#     \"time_unix\",\n",
    "#     \"minute\",\n",
    "#     \"hour\",\n",
    "#     \"day\",\n",
    "#     \"month\",\n",
    "# ]\n",
    "# scaler = StandardScaler()\n",
    "# for col in df.columns:\n",
    "#     if col not in do_not_scale_columns:\n",
    "#         df[col] = scaler.fit_transform(df[[col]])\n",
    "\n",
    "\n",
    "# X = df.drop(\"color_change\", axis=1)\n",
    "# y = df[\"color_change\"]\n",
    "\n",
    "# df.tail()\n",
    "# duplicate features\n",
    "# duplicated_features = df.columns.duplicated()\n",
    "# print(\"Duplicate Features: \", df.columns[duplicated_features])\n",
    "# Total features\n",
    "# print(\"Total features in DataFrame: \", df.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Selection Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[367], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Re-scale the data to include the new feature\u001b[39;00m\n\u001b[0;32m      2\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 3\u001b[0m X_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X)\n\u001b[0;32m      5\u001b[0m \u001b[39m# feature selection\u001b[39;00m\n\u001b[0;32m      6\u001b[0m selector \u001b[39m=\u001b[39m SelectKBest(score_func\u001b[39m=\u001b[39mf_classif, k\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Re-scale the data to include the new feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# feature selection\n",
    "selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "selector.fit(X_scaled, y)\n",
    "\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "features_df_new = X.iloc[:, cols]\n",
    "\n",
    "# Store the scores of each feature in a dictionary\n",
    "feature_scores = {\n",
    "    feature_name: score for feature_name, score in zip(X.columns, selector.scores_)\n",
    "}\n",
    "\n",
    "# Sort the dictionary by value in descending order and print the scores\n",
    "for feature_name, score in sorted(\n",
    "    feature_scores.items(), key=lambda item: item[1], reverse=True\n",
    "):\n",
    "    print(f\"{feature_name}: {score}\")\n",
    "\n",
    "# Now we can apply Logistic Regression and Random Forests on the new features_df_new\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, features_df_new, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "print(f\"\\nLogistic Regression CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, features_df_new, y, cv=tscv, scoring=\"f1\")\n",
    "\n",
    "print(f\"Random Forest CV F1 score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "X_array = X.values\n",
    "X_reshaped = X_array.reshape((X_array.shape[0], 1, X_array.shape[1]))\n",
    "\n",
    "# Call the function\n",
    "mean_f1_score = timeseries_cv_score(X_reshaped, y.values, n_splits=5)\n",
    "print(f\"\\nLSTM CV F1 score: {mean_f1_score}\")\n",
    "\n",
    "print(\"\\n\", features_df_new.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[227], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m log_reg \u001b[39m=\u001b[39m LogisticRegression(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Cross-validation\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m cv_scores \u001b[39m=\u001b[39m cross_val_score(log_reg, X, y, cv\u001b[39m=\u001b[39mtscv, scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mroc_auc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLogistic Regression CV ROC AUC score: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(cv_scores)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Random Forest\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=500)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(log_reg, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Logistic Regression CV ROC AUC score: {np.mean(cv_scores)}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf, X, y, cv=tscv, scoring=\"roc_auc\")\n",
    "\n",
    "print(f\"Random Forest CV ROC AUC score: {np.mean(cv_scores)}\")\n",
    "\n",
    "print(\"\\n\", X.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 812us/step\n",
      "42/42 [==============================] - 0s 842us/step\n",
      "42/42 [==============================] - 0s 781us/step\n",
      "42/42 [==============================] - 0s 842us/step\n",
      "42/42 [==============================] - 0s 720us/step\n",
      "\n",
      "LSTM CV ROC AUC score: (0.6953945419128389, 0.4942367515727568)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input to be 3D [samples, timesteps, features]\n",
    "X_array = X.values\n",
    "X_reshaped = X_array.reshape((X_array.shape[0], 1, X_array.shape[1]))\n",
    "\n",
    "# Call the function\n",
    "mean_auc_score = timeseries_cv_score(X_reshaped, y.values, n_splits=5)\n",
    "print(f\"\\nLSTM CV ROC AUC score: {mean_auc_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
